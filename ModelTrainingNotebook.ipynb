{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13415296,"sourceType":"datasetVersion","datasetId":8514330}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install a set of compatible versions. Run in one cell.\n!pip install -q \"huggingface_hub==0.28.1\" \"transformers==4.41.0\" \"accelerate==0.20.3\" ftfy safetensors\n!pip install -q datasets torchvision pillow tqdm requests pandas\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:42:50.079330Z","iopub.execute_input":"2025-10-18T03:42:50.079585Z","iopub.status.idle":"2025-10-18T03:44:40.976249Z","shell.execute_reply.started":"2025-10-18T03:42:50.079563Z","shell.execute_reply":"2025-10-18T03:44:40.974596Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npeft 0.16.0 requires accelerate>=0.21.0, but you have accelerate 0.20.3 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import huggingface_hub, transformers, accelerate, torch\nprint(\"huggingface_hub\", huggingface_hub.__version__)\nprint(\"transformers\", transformers.__version__)\nprint(\"accelerate\", accelerate.__version__)\nprint(\"torch\", torch.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:44:48.854855Z","iopub.execute_input":"2025-10-18T03:44:48.855235Z","iopub.status.idle":"2025-10-18T03:45:18.945435Z","shell.execute_reply.started":"2025-10-18T03:44:48.855176Z","shell.execute_reply":"2025-10-18T03:45:18.944212Z"}},"outputs":[{"name":"stderr","text":"2025-10-18 03:44:57.703954: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760759097.952863      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760759098.029460      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"huggingface_hub 0.28.1\ntransformers 4.41.0\naccelerate 0.20.3\ntorch 2.6.0+cu124\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from pathlib import Path\nimport json\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport os\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch\nimport pandas as pd\n\nprint(\"torch:\", torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:46:25.712660Z","iopub.execute_input":"2025-10-18T03:46:25.713161Z","iopub.status.idle":"2025-10-18T03:46:26.528719Z","shell.execute_reply.started":"2025-10-18T03:46:25.713131Z","shell.execute_reply":"2025-10-18T03:46:26.527560Z"}},"outputs":[{"name":"stdout","text":"torch: 2.6.0+cu124\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# If running on Kaggle, dataset may be under /kaggle/input\nPOSSIBLE_LOCAL = Path(\"/kaggle/input\")\nif POSSIBLE_LOCAL.exists() and any(POSSIBLE_LOCAL.iterdir()):\n    # pick CSV under /kaggle/input if present\n    csvs = list(POSSIBLE_LOCAL.rglob(\"*.csv\"))\n    if csvs:\n        DATA_FILE = csvs[0]\n    else:\n        # fallback to user-provided common path\n        DATA_FILE = Path(\"/kaggle/working/ikarus.csv\")\nelse:\n    # local notebook path where you uploaded the file\n    DATA_FILE = Path(\"/kaggle/working/intern_data_ikarus.csv\")\n    # if not present, check /mnt/data\n    if not DATA_FILE.exists():\n        alt = Path(\"/mnt/data/intern_data_ikarus.csv\")\n        if alt.exists():\n            DATA_FILE = alt\n\nif not DATA_FILE.exists():\n    raise FileNotFoundError(f\"Dataset file not found. Expected at {DATA_FILE} or /kaggle/input/* . Please upload the CSV to the notebook.\")\nprint(\"Using dataset:\", DATA_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:46:29.604379Z","iopub.execute_input":"2025-10-18T03:46:29.605945Z","iopub.status.idle":"2025-10-18T03:46:29.620422Z","shell.execute_reply.started":"2025-10-18T03:46:29.605909Z","shell.execute_reply":"2025-10-18T03:46:29.619016Z"}},"outputs":[{"name":"stdout","text":"Using dataset: /kaggle/input/ikarus/intern_data_ikarus.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df = pd.read_csv(DATA_FILE, low_memory=False)\nprint(\"Loaded dataframe shape:\", df.shape)\nprint(\"Columns:\", df.columns.tolist())\n\n# Required columns the assignment mentions\nrequired = [\"title\",\"brand\",\"description\",\"price\",\"categories\",\"images\",\"manufacturer\",\n            \"package dimensions\",\"country_of_origin\",\"material\",\"color\",\"uniq_id\"]\nmissing = [c for c in required if c not in df.columns]\nif missing:\n    print(\"Warning - these required columns are missing from CSV:\", missing)\nelse:\n    print(\"All required columns are present.\")\n\n# Show a few rows for manual check\ndisplay(df.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:46:33.329108Z","iopub.execute_input":"2025-10-18T03:46:33.329490Z","iopub.status.idle":"2025-10-18T03:46:33.388996Z","shell.execute_reply.started":"2025-10-18T03:46:33.329464Z","shell.execute_reply":"2025-10-18T03:46:33.387674Z"}},"outputs":[{"name":"stdout","text":"Loaded dataframe shape: (312, 12)\nColumns: ['title', 'brand', 'description', 'price', 'categories', 'images', 'manufacturer', 'package_dimensions', 'country_of_origin', 'material', 'color', 'uniq_id']\nWarning - these required columns are missing from CSV: ['package dimensions']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                               title    brand  \\\n0  GOYMFK 1pc Free Standing Shoe Rack, Multi-laye...   GOYMFK   \n1  subrtex Leather ding Room, Dining Chairs Set o...  subrtex   \n2  Plant Repotting Mat MUYETOL Waterproof Transpl...  MUYETOL   \n\n                                         description   price  \\\n0  multiple shoes, coats, hats, and other items E...  $24.99   \n1                     subrtex Dining chairs Set of 2     NaN   \n2                                                NaN   $5.98   \n\n                                          categories  \\\n0  ['Home & Kitchen', 'Storage & Organization', '...   \n1  ['Home & Kitchen', 'Furniture', 'Dining Room F...   \n2  ['Patio, Lawn & Garden', 'Outdoor Décor', 'Doo...   \n\n                                              images           manufacturer  \\\n0  ['https://m.media-amazon.com/images/I/416WaLx1...                 GOYMFK   \n1  ['https://m.media-amazon.com/images/I/31SejUEW...  Subrtex Houseware INC   \n2  ['https://m.media-amazon.com/images/I/41RgefVq...                MUYETOL   \n\n         package_dimensions country_of_origin      material  color  \\\n0  2.36\"D x 7.87\"W x 21.6\"H             China         Metal  White   \n1      18.5\"D x 16\"W x 35\"H               NaN        Sponge  Black   \n2           26.8\"L x 26.8\"W               NaN  Polyethylene  Green   \n\n                                uniq_id  \n0  02593e81-5c09-5069-8516-b0b29f439ded  \n1  5938d217-b8c5-5d3e-b1cf-e28e340f292e  \n2  b2ede786-3f51-5a45-9a5b-bcf856958cd8  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>brand</th>\n      <th>description</th>\n      <th>price</th>\n      <th>categories</th>\n      <th>images</th>\n      <th>manufacturer</th>\n      <th>package_dimensions</th>\n      <th>country_of_origin</th>\n      <th>material</th>\n      <th>color</th>\n      <th>uniq_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GOYMFK 1pc Free Standing Shoe Rack, Multi-laye...</td>\n      <td>GOYMFK</td>\n      <td>multiple shoes, coats, hats, and other items E...</td>\n      <td>$24.99</td>\n      <td>['Home &amp; Kitchen', 'Storage &amp; Organization', '...</td>\n      <td>['https://m.media-amazon.com/images/I/416WaLx1...</td>\n      <td>GOYMFK</td>\n      <td>2.36\"D x 7.87\"W x 21.6\"H</td>\n      <td>China</td>\n      <td>Metal</td>\n      <td>White</td>\n      <td>02593e81-5c09-5069-8516-b0b29f439ded</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>subrtex Leather ding Room, Dining Chairs Set o...</td>\n      <td>subrtex</td>\n      <td>subrtex Dining chairs Set of 2</td>\n      <td>NaN</td>\n      <td>['Home &amp; Kitchen', 'Furniture', 'Dining Room F...</td>\n      <td>['https://m.media-amazon.com/images/I/31SejUEW...</td>\n      <td>Subrtex Houseware INC</td>\n      <td>18.5\"D x 16\"W x 35\"H</td>\n      <td>NaN</td>\n      <td>Sponge</td>\n      <td>Black</td>\n      <td>5938d217-b8c5-5d3e-b1cf-e28e340f292e</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Plant Repotting Mat MUYETOL Waterproof Transpl...</td>\n      <td>MUYETOL</td>\n      <td>NaN</td>\n      <td>$5.98</td>\n      <td>['Patio, Lawn &amp; Garden', 'Outdoor Décor', 'Doo...</td>\n      <td>['https://m.media-amazon.com/images/I/41RgefVq...</td>\n      <td>MUYETOL</td>\n      <td>26.8\"L x 26.8\"W</td>\n      <td>NaN</td>\n      <td>Polyethylene</td>\n      <td>Green</td>\n      <td>b2ede786-3f51-5a45-9a5b-bcf856958cd8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Normalize column names if data uses slight variations\n# choose exact column names from df where possible\ncols_lower = {c.lower(): c for c in df.columns}\ndef pick(colname):\n    return cols_lower.get(colname, None)\n\ntitle_col = pick(\"title\") or pick(\"name\")\ndesc_col = pick(\"description\")\nimages_col = pick(\"images\") or pick(\"image\") or pick(\"image_url\") or pick(\"imagepath\")\nuniq_col = pick(\"uniq_id\") or pick(\"id\") or pick(\"product_id\")\n\nif uniq_col is None:\n    # fallback to row index as id\n    df[\"uniq_id\"] = df.index.astype(str)\n    uniq_col = \"uniq_id\"\n\nprint(\"Mapped columns -> uniq:\", uniq_col, \"title:\", title_col, \"desc:\", desc_col, \"images:\", images_col)\n\n# build products list\nproducts = []\nfor _, row in df.iterrows():\n    pid = str(row.get(uniq_col)) if pd.notna(row.get(uniq_col)) else None\n    title = str(row.get(title_col)) if title_col and pd.notna(row.get(title_col)) else \"\"\n    desc = str(row.get(desc_col)) if desc_col and pd.notna(row.get(desc_col)) else \"\"\n    images_val = None\n    if images_col and pd.notna(row.get(images_col)):\n        images_val = str(row.get(images_col))\n    # preserve all metadata but do not drop important columns\n    meta = {k: (None if pd.isna(v) else v) for k, v in row.to_dict().items()}\n    products.append({\"id\": pid, \"title\": title, \"description\": desc, \"images_field\": images_val, \"meta\": meta})\nproducts = [p for p in products if p[\"id\"] is not None]\nprint(\"Prepared products count:\", len(products))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:48:28.544345Z","iopub.execute_input":"2025-10-18T03:48:28.544879Z","iopub.status.idle":"2025-10-18T03:48:28.592527Z","shell.execute_reply.started":"2025-10-18T03:48:28.544847Z","shell.execute_reply":"2025-10-18T03:48:28.591347Z"}},"outputs":[{"name":"stdout","text":"Mapped columns -> uniq: uniq_id title: title desc: description images: images\nPrepared products count: 312\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"WORK_DIR = Path(\"/kaggle/working\")\nIMAGES_DIR = WORK_DIR / \"images\"\nIMAGES_DIR.mkdir(parents=True, exist_ok=True)\n\ndef download_or_resolve_image(image_val, pid):\n    if image_val is None:\n        return None\n    image_val = str(image_val).strip()\n    if image_val.lower().startswith(\"http\"):\n        try:\n            resp = requests.get(image_val, timeout=15)\n            if resp.status_code == 200:\n                img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n                outp = IMAGES_DIR / f\"{pid}.jpg\"\n                img.save(outp)\n                return str(outp)\n            else:\n                return None\n        except Exception:\n            return None\n    else:\n        # treat as local path inside mounted dataset (try common locations)\n        p = Path(image_val)\n        if not p.exists():\n            # try relative search inside /kaggle/input\n            possible = list(Path(\"/kaggle/input\").rglob(Path(image_val).name)) if Path(\"/kaggle/input\").exists() else []\n            if possible:\n                p = possible[0]\n        if p.exists():\n            try:\n                img = Image.open(p).convert(\"RGB\")\n                outp = IMAGES_DIR / f\"{pid}.jpg\"\n                img.save(outp)\n                return str(outp)\n            except Exception:\n                return None\n        else:\n            return None\n\nfor p in tqdm(products, desc=\"resolving images\"):\n    val = p.get(\"images_field\")\n    local = download_or_resolve_image(val, p[\"id\"])\n    p[\"local_image\"] = local\n\nnum_img = sum(1 for p in products if p[\"local_image\"])\nprint(f\"Images available for {num_img}/{len(products)} products\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:48:33.532126Z","iopub.execute_input":"2025-10-18T03:48:33.532951Z","iopub.status.idle":"2025-10-18T03:48:34.063424Z","shell.execute_reply.started":"2025-10-18T03:48:33.532907Z","shell.execute_reply":"2025-10-18T03:48:34.062274Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"resolving images:   0%|          | 0/312 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfb672cde0f04c678c14c65198369a0c"}},"metadata":{}},{"name":"stdout","text":"Images available for 0/312 products\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from collections import Counter\nvals = [p.get(\"images_field\") for p in products]\n# show 30 samples (first non-null and last non-null)\nnon_null = [v for v in vals if v is not None and str(v).strip() != \"\"]\nprint(\"Total rows:\", len(products))\nprint(\"Non-empty images_field count:\", len(non_null))\nprint(\"Sample of first 30 non-empty values:\")\nfor i, v in enumerate(non_null[:30]):\n    print(i+1, \"->\", repr(v)[:300])\n# show common prefixes or separators\nseps = Counter()\nfor v in non_null[:200]:\n    s = str(v)\n    if \"|\" in s:\n        seps[\"pipe |\"] += 1\n    if \";\" in s:\n        seps[\"semicolon ;\"] += 1\n    if \",\" in s and \"http\" in s:\n        seps[\"comma , (with http)\"] += 1\n    if s.strip().startswith(\"[\") and \"http\" in s:\n        seps[\"looks like list\"] += 1\n    if \"http\" in s:\n        seps[\"contains http\"] += 1\nprint(\"Heuristic separators / features (counts):\", dict(seps))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:48:40.100072Z","iopub.execute_input":"2025-10-18T03:48:40.100413Z","iopub.status.idle":"2025-10-18T03:48:40.112114Z","shell.execute_reply.started":"2025-10-18T03:48:40.100391Z","shell.execute_reply":"2025-10-18T03:48:40.110930Z"}},"outputs":[{"name":"stdout","text":"Total rows: 312\nNon-empty images_field count: 312\nSample of first 30 non-empty values:\n1 -> \"['https://m.media-amazon.com/images/I/416WaLx10jL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41kuxipTsuL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51T9x4yZd3L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/61w6ifIrCpL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n2 -> \"['https://m.media-amazon.com/images/I/31SejUEWY7L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41mr+A9JmbL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41JjrWgA0XL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41ie5FbnfkL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/4\n3 -> \"['https://m.media-amazon.com/images/I/41RgefVq70L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/414SPEuzxlL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51gknsPKCHL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51mcO9+8GLL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n4 -> \"['https://m.media-amazon.com/images/I/61vz1IglerL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41KiuOZ9IlL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/61N7oADRxIL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51P410C3CpL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n5 -> \"['https://m.media-amazon.com/images/I/41p4d4VJnNL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51nycdOa5kL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/416f7xG0D6L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51ayCcN+W7L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n6 -> \"['https://m.media-amazon.com/images/I/41zMuj2wvvL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41h0FQWG5IL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41thgR6LltL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41oC3JqceaL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n7 -> \"['https://m.media-amazon.com/images/I/41ixgM73DgL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41WLiP-jOaL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41i3PONnKTL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/31U4qzySmeL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n8 -> \"['https://m.media-amazon.com/images/I/416WaLx10jL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41kuxipTsuL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51T9x4yZd3L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/61w6ifIrCpL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n9 -> \"['https://m.media-amazon.com/images/I/31SejUEWY7L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41mr+A9JmbL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41JjrWgA0XL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41ie5FbnfkL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/4\n10 -> \"['https://m.media-amazon.com/images/I/41RgefVq70L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/414SPEuzxlL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51gknsPKCHL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51mcO9+8GLL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n11 -> \"['https://m.media-amazon.com/images/I/61vz1IglerL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41KiuOZ9IlL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/61N7oADRxIL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51P410C3CpL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n12 -> \"['https://m.media-amazon.com/images/I/41p4d4VJnNL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51nycdOa5kL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/416f7xG0D6L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51ayCcN+W7L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n13 -> \"['https://m.media-amazon.com/images/I/41zMuj2wvvL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41h0FQWG5IL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41thgR6LltL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41oC3JqceaL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n14 -> \"['https://m.media-amazon.com/images/I/41ixgM73DgL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41WLiP-jOaL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41i3PONnKTL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/31U4qzySmeL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n15 -> \"['https://m.media-amazon.com/images/I/41IzLmM91FL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51J83+zKGCL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51W89bfhZgL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51GOSVUqhwL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n16 -> \"['https://m.media-amazon.com/images/I/41rMElFrXBL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41-yJNTT1FL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41KihITH3RL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41rsTeqjQzL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/4\n17 -> \"['https://m.media-amazon.com/images/I/21opezr3bUL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/31HlJoUxwcL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41POu+Y--iL._SS522_.jpg']\"\n18 -> \"['https://m.media-amazon.com/images/I/41HxUoRXloL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41O0tbqtVfL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41apCz-5USL._SS522_.jpg']\"\n19 -> \"['https://m.media-amazon.com/images/I/41YrmN-yOEL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51ySXwNCU7L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/418lxFzHARL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51HMXH0NWSL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/3\n20 -> \"['https://m.media-amazon.com/images/I/31rrpY5EJOL._SS522_.jpg']\"\n21 -> \"['https://m.media-amazon.com/images/I/31cbTc2VhaL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51iLRDni3SL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/511IENAhEfL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51CwGvaB36L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n22 -> \"['https://m.media-amazon.com/images/I/31l+OVMKBFL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/510aFU231KL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/31hz-89KKNL._SS522_.jpg']\"\n23 -> \"['https://m.media-amazon.com/images/I/51Zn-AivGrL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51KczSA8RxL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51-LvXau6sL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51SsV7oKOHL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n24 -> \"['https://m.media-amazon.com/images/I/31XhtLE1F1L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/31P5IpUuZpS._SS522_.jpg ', ' https://m.media-amazon.com/images/I/31+AyqdiGjS._SS522_.jpg ', ' https://m.media-amazon.com/images/I/31d0QTUZ5GL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/4\n25 -> \"['https://m.media-amazon.com/images/I/51V1IWRrXHL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/514YwvmLkpL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51oPAizyXaL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51kJA6JeRrL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/5\n26 -> \"['https://m.media-amazon.com/images/I/51-GsdoM+IS._SS522_.jpg ', ' https://m.media-amazon.com/images/I/31A+MlJNPPS._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51NN55v6ffS._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41fFYNH5xvS._SS522_.jpg ', ' https://m.media-amazon.com/images/I/4\n27 -> \"['https://m.media-amazon.com/images/I/41hYmELREGL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41VFJB73P8L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/31EVYkDak+L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41kqUqbzHdL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/4\n28 -> \"['https://m.media-amazon.com/images/I/41O7mY3lUvL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41SeqOInsyL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51s2LL0DTML._SS522_.jpg ', ' https://m.media-amazon.com/images/I/414XCaSNIDL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/4\n29 -> \"['https://m.media-amazon.com/images/I/51sai4l5ttL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/516siFpFIhL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/4168X67EeuL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41caGwmuxBL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/4\n30 -> \"['https://m.media-amazon.com/images/I/31eBuhJ0NDL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/517bw6-aFZL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/31bj4zvq41L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41Lt1Sj4TUL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/4\nHeuristic separators / features (counts): {'comma , (with http)': 194, 'looks like list': 200, 'contains http': 200}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import re, json\nfrom pathlib import Path\n\n# extraction helper tuned for the dataset format\nURL_REGEX = re.compile(r\"https?://[^\\s'\\\",]+\")  # stop at space, quote or comma\n\ndef extract_first_candidate(s):\n    if s is None:\n        return None\n    s = str(s).strip()\n    if not s:\n        return None\n    # Try regex first (will find the first http... in the string)\n    m = URL_REGEX.search(s)\n    if m:\n        return m.group(0)\n    # handle single-quoted python-list string: \"['url1','url2']\"\n    # remove surrounding quotes if present\n    if (s.startswith(\"'\") and s.endswith(\"'\")) or (s.startswith('\"') and s.endswith('\"')):\n        s2 = s[1:-1].strip()\n    else:\n        s2 = s\n    # replace single quotes with double to try JSON loads\n    try:\n        if s2.startswith(\"[\") and s2.endswith(\"]\"):\n            # convert single quotes -> double quotes safely for typical cases\n            json_like = s2.replace(\"'\", '\"')\n            parsed = json.loads(json_like)\n            if isinstance(parsed, (list, tuple)) and parsed:\n                # return first non-empty string trimmed\n                for item in parsed:\n                    if item and str(item).strip():\n                        return str(item).strip()\n    except Exception:\n        pass\n    # fallback: split on common separators\n    for sep in [\"|\", \";\", \",\"]:\n        if sep in s2:\n            parts = [x.strip().strip(\"'\").strip('\"') for x in s2.split(sep) if x.strip()]\n            if parts:\n                return parts[0]\n    return s2\n\n# show first 20 parsed candidates\nsamples = [p.get(\"images_field\") for p in products][:50]\nfor i, s in enumerate(samples[:20]):\n    print(i+1, \"raw->\", repr(s)[:200])\n    print(\"   parsed->\", extract_first_candidate(s))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:48:45.840133Z","iopub.execute_input":"2025-10-18T03:48:45.840523Z","iopub.status.idle":"2025-10-18T03:48:45.854302Z","shell.execute_reply.started":"2025-10-18T03:48:45.840495Z","shell.execute_reply":"2025-10-18T03:48:45.852779Z"}},"outputs":[{"name":"stdout","text":"1 raw-> \"['https://m.media-amazon.com/images/I/416WaLx10jL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41kuxipTsuL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51T9x4yZd3L._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/416WaLx10jL._SS522_.jpg\n2 raw-> \"['https://m.media-amazon.com/images/I/31SejUEWY7L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41mr+A9JmbL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41JjrWgA0XL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/31SejUEWY7L._SS522_.jpg\n3 raw-> \"['https://m.media-amazon.com/images/I/41RgefVq70L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/414SPEuzxlL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51gknsPKCHL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/41RgefVq70L._SS522_.jpg\n4 raw-> \"['https://m.media-amazon.com/images/I/61vz1IglerL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41KiuOZ9IlL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/61N7oADRxIL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/61vz1IglerL._SS522_.jpg\n5 raw-> \"['https://m.media-amazon.com/images/I/41p4d4VJnNL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51nycdOa5kL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/416f7xG0D6L._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/41p4d4VJnNL._SS522_.jpg\n6 raw-> \"['https://m.media-amazon.com/images/I/41zMuj2wvvL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41h0FQWG5IL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41thgR6LltL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/41zMuj2wvvL._SS522_.jpg\n7 raw-> \"['https://m.media-amazon.com/images/I/41ixgM73DgL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41WLiP-jOaL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41i3PONnKTL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/41ixgM73DgL._SS522_.jpg\n8 raw-> \"['https://m.media-amazon.com/images/I/416WaLx10jL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41kuxipTsuL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51T9x4yZd3L._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/416WaLx10jL._SS522_.jpg\n9 raw-> \"['https://m.media-amazon.com/images/I/31SejUEWY7L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41mr+A9JmbL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41JjrWgA0XL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/31SejUEWY7L._SS522_.jpg\n10 raw-> \"['https://m.media-amazon.com/images/I/41RgefVq70L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/414SPEuzxlL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51gknsPKCHL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/41RgefVq70L._SS522_.jpg\n11 raw-> \"['https://m.media-amazon.com/images/I/61vz1IglerL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41KiuOZ9IlL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/61N7oADRxIL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/61vz1IglerL._SS522_.jpg\n12 raw-> \"['https://m.media-amazon.com/images/I/41p4d4VJnNL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51nycdOa5kL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/416f7xG0D6L._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/41p4d4VJnNL._SS522_.jpg\n13 raw-> \"['https://m.media-amazon.com/images/I/41zMuj2wvvL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41h0FQWG5IL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41thgR6LltL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/41zMuj2wvvL._SS522_.jpg\n14 raw-> \"['https://m.media-amazon.com/images/I/41ixgM73DgL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41WLiP-jOaL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41i3PONnKTL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/41ixgM73DgL._SS522_.jpg\n15 raw-> \"['https://m.media-amazon.com/images/I/41IzLmM91FL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51J83+zKGCL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51W89bfhZgL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/41IzLmM91FL._SS522_.jpg\n16 raw-> \"['https://m.media-amazon.com/images/I/41rMElFrXBL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41-yJNTT1FL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41KihITH3RL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/41rMElFrXBL._SS522_.jpg\n17 raw-> \"['https://m.media-amazon.com/images/I/21opezr3bUL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/31HlJoUxwcL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41POu+Y--iL._SS522_.jpg']\"\n   parsed-> https://m.media-amazon.com/images/I/21opezr3bUL._SS522_.jpg\n18 raw-> \"['https://m.media-amazon.com/images/I/41HxUoRXloL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41O0tbqtVfL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/41apCz-5USL._SS522_.jpg']\"\n   parsed-> https://m.media-amazon.com/images/I/41HxUoRXloL._SS522_.jpg\n19 raw-> \"['https://m.media-amazon.com/images/I/41YrmN-yOEL._SS522_.jpg ', ' https://m.media-amazon.com/images/I/51ySXwNCU7L._SS522_.jpg ', ' https://m.media-amazon.com/images/I/418lxFzHARL._SS522_.jpg ', ' ht\n   parsed-> https://m.media-amazon.com/images/I/41YrmN-yOEL._SS522_.jpg\n20 raw-> \"['https://m.media-amazon.com/images/I/31rrpY5EJOL._SS522_.jpg']\"\n   parsed-> https://m.media-amazon.com/images/I/31rrpY5EJOL._SS522_.jpg\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import requests\nfrom io import BytesIO\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport re, json\nfrom pathlib import Path\n\nWORK_DIR = Path(\"/kaggle/working\")\nIMAGES_DIR = WORK_DIR / \"images\"\nIMAGES_DIR.mkdir(parents=True, exist_ok=True)\n\nURL_REGEX = re.compile(r\"https?://[^\\s'\\\",]+\")\n\ndef extract_first_url_from_field(s):\n    \"\"\"Return first URL or local token from images_field.\"\"\"\n    if s is None:\n        return None\n    s = str(s).strip()\n    if s == \"\" or s.lower() in (\"none\",\"nan\",\"[]\"):\n        return None\n    # quick regex fetch first http(s) URL\n    m = URL_REGEX.search(s)\n    if m:\n        return m.group(0).strip()\n    # remove outer quotes\n    if (s.startswith('\"') and s.endswith('\"')) or (s.startswith(\"'\") and s.endswith(\"'\")):\n        inner = s[1:-1].strip()\n    else:\n        inner = s\n    # try JSON-like: convert single quotes to double quotes if looks like list\n    if inner.startswith(\"[\") and inner.endswith(\"]\"):\n        try:\n            candidate_list = json.loads(inner.replace(\"'\", '\"'))\n            if isinstance(candidate_list, (list,tuple)) and candidate_list:\n                # find first http-like element\n                for it in candidate_list:\n                    if it and isinstance(it, str) and \"http\" in it:\n                        return it.strip()\n                # otherwise return first element as fallback\n                return str(candidate_list[0]).strip()\n        except Exception:\n            pass\n    # split on common separators\n    for sep in [\"|\",\";\",\" ,\",\",\"]:\n        if sep in inner:\n            parts = [x.strip().strip(\"'\").strip('\"') for x in inner.split(sep) if x.strip()]\n            for p in parts:\n                if p and \"http\" in p:\n                    return p\n            if parts:\n                return parts[0]\n    return inner\n\ndef download_image_from_url(url, out_path, timeout=20):\n    \"\"\"Download and save an image; return True on success.\"\"\"\n    headers = {\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36\"}\n    try:\n        resp = requests.get(url, timeout=timeout, headers=headers)\n        if resp.status_code != 200:\n            return False\n        img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n        img.save(out_path)\n        return True\n    except Exception:\n        return False\n\n# Run resolver + downloader for all products; download first URL only\ndownloaded = 0\nfailed = 0\nfor p in tqdm(products, desc=\"resolving & downloading images\"):\n    raw = p.get(\"images_field\")\n    first = extract_first_url_from_field(raw)\n    if not first:\n        p[\"local_image\"] = None\n        continue\n    # if candidate looks like URL -> download\n    if str(first).lower().startswith(\"http\"):\n        outp = IMAGES_DIR / f\"{p['id']}.jpg\"\n        ok = download_image_from_url(first, outp)\n        if ok:\n            p[\"local_image\"] = str(outp)\n            downloaded += 1\n        else:\n            p[\"local_image\"] = None\n            failed += 1\n    else:\n        # treat as local filename inside /kaggle/input\n        cand = Path(first)\n        found = None\n        if cand.exists():\n            found = cand\n        else:\n            # search under /kaggle/input\n            if Path(\"/kaggle/input\").exists():\n                possible = list(Path(\"/kaggle/input\").rglob(cand.name))\n                if possible:\n                    found = possible[0]\n        if found:\n            try:\n                img = Image.open(found).convert(\"RGB\")\n                outp = IMAGES_DIR / f\"{p['id']}.jpg\"\n                img.save(outp)\n                p[\"local_image\"] = str(outp)\n                downloaded += 1\n            except Exception:\n                p[\"local_image\"] = None\n                failed += 1\n        else:\n            p[\"local_image\"] = None\n            failed += 1\n\nprint(\"Downloaded:\", downloaded, \"Failed attempts:\", failed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:48:50.540335Z","iopub.execute_input":"2025-10-18T03:48:50.540676Z","iopub.status.idle":"2025-10-18T03:49:43.372073Z","shell.execute_reply.started":"2025-10-18T03:48:50.540653Z","shell.execute_reply":"2025-10-18T03:49:43.370979Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"resolving & downloading images:   0%|          | 0/312 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d654473e784ee9acd9647bad4b4494"}},"metadata":{}},{"name":"stdout","text":"Downloaded: 312 Failed attempts: 0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"\nnum_img = sum(1 for p in products if p.get(\"local_image\"))\nprint(f\"Images available for {num_img}/{len(products)} products\")\n\n# show 10 sample local_image paths\ncnt = 0\nfor p in products:\n    if p.get(\"local_image\"):\n        print(p[\"id\"], \"->\", p[\"local_image\"])\n        cnt += 1\n        if cnt >= 10:\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:52:13.241426Z","iopub.execute_input":"2025-10-18T03:52:13.242763Z","iopub.status.idle":"2025-10-18T03:52:13.254276Z","shell.execute_reply.started":"2025-10-18T03:52:13.242709Z","shell.execute_reply":"2025-10-18T03:52:13.252154Z"}},"outputs":[{"name":"stdout","text":"Images available for 312/312 products\n02593e81-5c09-5069-8516-b0b29f439ded -> /kaggle/working/images/02593e81-5c09-5069-8516-b0b29f439ded.jpg\n5938d217-b8c5-5d3e-b1cf-e28e340f292e -> /kaggle/working/images/5938d217-b8c5-5d3e-b1cf-e28e340f292e.jpg\nb2ede786-3f51-5a45-9a5b-bcf856958cd8 -> /kaggle/working/images/b2ede786-3f51-5a45-9a5b-bcf856958cd8.jpg\n8fd9377b-cfa6-5f10-835c-6b8eca2816b5 -> /kaggle/working/images/8fd9377b-cfa6-5f10-835c-6b8eca2816b5.jpg\nbdc9aa30-9439-50dc-8e89-213ea211d66a -> /kaggle/working/images/bdc9aa30-9439-50dc-8e89-213ea211d66a.jpg\n20da3703-26f1-53e5-aa0b-a8104527d1bb -> /kaggle/working/images/20da3703-26f1-53e5-aa0b-a8104527d1bb.jpg\naba4138e-6401-52ca-a099-02e30b638db4 -> /kaggle/working/images/aba4138e-6401-52ca-a099-02e30b638db4.jpg\n02593e81-5c09-5069-8516-b0b29f439ded -> /kaggle/working/images/02593e81-5c09-5069-8516-b0b29f439ded.jpg\n5938d217-b8c5-5d3e-b1cf-e28e340f292e -> /kaggle/working/images/5938d217-b8c5-5d3e-b1cf-e28e340f292e.jpg\nb2ede786-3f51-5a45-9a5b-bcf856958cd8 -> /kaggle/working/images/b2ede786-3f51-5a45-9a5b-bcf856958cd8.jpg\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Load CLIP model + helper functions\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom PIL import Image\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\nMODEL_NAME = \"openai/clip-vit-base-patch32\"\nprint(\"Loading CLIP model:\", MODEL_NAME)\nmodel = CLIPModel.from_pretrained(MODEL_NAME).to(device)\nprocessor = CLIPProcessor.from_pretrained(MODEL_NAME)\nmodel.eval()\n\n# helpers for batching & normalization\ndef l2_normalize_rows(a: np.ndarray):\n    if a.size == 0:\n        return a\n    norms = np.linalg.norm(a, axis=1, keepdims=True)\n    norms[norms == 0] = 1.0\n    return a / norms\n\ndef compute_text_embeddings(texts, batch_size=64):\n    all_emb = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=\"text batches\"):\n            batch = texts[i:i+batch_size]\n            inputs = processor(text=batch, return_tensors=\"pt\", padding=True).to(device)\n            emb = model.get_text_features(**inputs)   # (B, D)\n            all_emb.append(emb.cpu().numpy())\n    if not all_emb:\n        return np.zeros((0, model.config.projection_dim), dtype=np.float32)\n    return np.vstack(all_emb).astype(np.float32)\n\ndef compute_image_embeddings(paths, batch_size=32):\n    all_emb = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(paths), batch_size), desc=\"image batches\"):\n            batch_paths = paths[i:i+batch_size]\n            images = []\n            for p in batch_paths:\n                if p is None:\n                    images.append(Image.new(\"RGB\",(224,224),color=(128,128,128)))\n                else:\n                    try:\n                        img = Image.open(p).convert(\"RGB\")\n                    except Exception:\n                        img = Image.new(\"RGB\",(224,224),color=(128,128,128))\n                    images.append(img)\n            inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n            emb = model.get_image_features(**inputs)  # (B, D)\n            all_emb.append(emb.cpu().numpy())\n    if not all_emb:\n        return np.zeros((0, model.config.projection_dim), dtype=np.float32)\n    return np.vstack(all_emb).astype(np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:52:16.658568Z","iopub.execute_input":"2025-10-18T03:52:16.658945Z","iopub.status.idle":"2025-10-18T03:52:22.734377Z","shell.execute_reply.started":"2025-10-18T03:52:16.658915Z","shell.execute_reply":"2025-10-18T03:52:22.733210Z"}},"outputs":[{"name":"stdout","text":"Device: cpu\nLoading CLIP model: openai/clip-vit-base-patch32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1dc3775a40f4f09be0598bc8380be7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2756be30924768a1b76cb68c4ab255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b70f8016780b409fac5cdae104619d46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4b17a21cc0f464f85faf8c2653217f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19533bdf76d54ee9a81b23ba46501358"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f2dc316f9b44265864d464083760ec4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0b848de41cf4fa5b3338b474b8d1ed2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce66b3d2d99404a82abeac31f603da1"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Chunk-aware text embedding + recompute embeddings / index\nimport math\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport torch\n\n# Access tokenizer and model\ntokenizer = processor.tokenizer\n# typical CLIP max length; fallback to 77\nmax_len = getattr(tokenizer, \"model_max_length\", None) or 77\nprint(\"Tokenizer max length:\", max_len)\n\n# Create chunks per text based on token length\ndef make_text_chunks(texts, max_tokens=max_len):\n    \"\"\"\n    For each text in texts list produce one or more chunks (strings), \n    and build a mapping from chunk index to original text index.\n    Returns: chunks_list, mapping (list of original indices for each chunk)\n    \"\"\"\n    chunks = []\n    mapping = []  # mapping[i_chunk] = index_of_original_text\n    for idx, txt in enumerate(texts):\n        if not txt:\n            # empty text -> one empty chunk\n            chunks.append(\"\")\n            mapping.append(idx)\n            continue\n        # quick token-length check\n        enc = tokenizer(txt, truncation=False, add_special_tokens=True)\n        length = len(enc[\"input_ids\"])\n        if length <= max_tokens:\n            chunks.append(txt)\n            mapping.append(idx)\n            continue\n        # else, split into word pieces approximately and produce chunks\n        words = txt.split()\n        cur_chunk_words = []\n        # iterate words, accumulate until tokenized length would exceed max_tokens-2\n        for w in words:\n            cur_chunk_words.append(w)\n            # check token count of current chunk\n            cur_txt = \" \".join(cur_chunk_words)\n            cur_len = len(tokenizer(cur_txt, truncation=False, add_special_tokens=True)[\"input_ids\"])\n            if cur_len >= max_tokens:\n                # back off last word to keep under limit\n                # if single word too long (rare), we still take it\n                if len(cur_chunk_words) == 1:\n                    chunks.append(cur_txt)\n                    mapping.append(idx)\n                    cur_chunk_words = []\n                else:\n                    # remove last word, finalize chunk\n                    last = cur_chunk_words.pop()\n                    chunk_txt = \" \".join(cur_chunk_words)\n                    chunks.append(chunk_txt)\n                    mapping.append(idx)\n                    # start new chunk with last word\n                    cur_chunk_words = [last]\n        # any remaining words -> final chunk\n        if cur_chunk_words:\n            chunks.append(\" \".join(cur_chunk_words))\n            mapping.append(idx)\n    return chunks, mapping\n\n# Build texts list again (just in case)\ntexts = [(p.get(\"title\",\"\") + \". \" + (p.get(\"description\") or \"\")) for p in products]\n\nprint(\"Preparing chunks for\", len(texts), \"texts...\")\nchunks, mapping = make_text_chunks(texts, max_tokens=max_len)\nprint(\"Total chunks produced:\", len(chunks))\n\n# Now compute embeddings for all chunks in batches\ndef compute_text_embeddings_for_chunks(chunks_list, batch_size=64):\n    all_emb = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(chunks_list), batch_size), desc=\"chunk batches\"):\n            batch = chunks_list[i:i+batch_size]\n            inputs = processor(text=batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).to(device)\n            emb = model.get_text_features(**inputs)  # (B, D)\n            all_emb.append(emb.cpu().numpy())\n    if not all_emb:\n        return np.zeros((0, model.config.projection_dim), dtype=np.float32)\n    return np.vstack(all_emb).astype(np.float32)\n\nchunk_emb = compute_text_embeddings_for_chunks(chunks, batch_size=64)\nprint(\"Chunk embeddings shape:\", chunk_emb.shape)\n\n# Average chunk embeddings per original text index\ndim = chunk_emb.shape[1]\nn_texts = len(texts)\ntext_emb_new = np.zeros((n_texts, dim), dtype=np.float32)\ncounts = np.zeros((n_texts,), dtype=np.int32)\nfor i_chunk, orig_idx in enumerate(mapping):\n    text_emb_new[orig_idx] += chunk_emb[i_chunk]\n    counts[orig_idx] += 1\n# avoid divide-by-zero\nfor i in range(n_texts):\n    if counts[i] > 0:\n        text_emb_new[i] = text_emb_new[i] / counts[i]\n    else:\n        # fallback: zero vector (shouldn't happen)\n        text_emb_new[i] = np.zeros((dim,), dtype=np.float32)\n\n# Normalize\ndef l2_normalize_rows_np(a):\n    norms = np.linalg.norm(a, axis=1, keepdims=True)\n    norms[norms==0] = 1.0\n    return a / norms\n\ntext_emb_new = l2_normalize_rows_np(text_emb_new)\nprint(\"Final text embeddings shape (per product):\", text_emb_new.shape)\n\n# Recompute image embeddings if not yet computed or if you want to reuse existing img_emb\n# If img_emb variable exists and has correct shape, we can reuse it. Otherwise compute:\ntry:\n    img_emb  # check existence\n    recompute_images = False\nexcept NameError:\n    recompute_images = True\n\nif recompute_images:\n    print(\"Computing image embeddings (images were not present in memory)...\")\n    img_emb = compute_image_embeddings(image_paths, batch_size=32)\n    img_emb = l2_normalize_rows_np(img_emb)\n    print(\"Image embeddings shape:\", img_emb.shape)\nelse:\n    print(\"Reusing existing img_emb with shape:\", img_emb.shape)\n\n# Combine embeddings exactly like before\nw_text = 0.6\nw_img = 0.4\n\ncombined = []\nids = []\nmetadata = {}\nmissing_image_count = 0\n\nfor i, p in enumerate(products):\n    te = text_emb_new[i] if text_emb_new.shape[0] > i else np.zeros((dim,), dtype=np.float32)\n    ie = img_emb[i] if img_emb.shape[0] > i else np.zeros((dim,), dtype=np.float32)\n    if p.get(\"local_image\") is None or (ie is None or ie.size == 0):\n        v = te\n        missing_image_count += 1\n    else:\n        v = (w_text * te + w_img * ie)\n    v = v / (np.linalg.norm(v) + 1e-12)\n    combined.append(v.astype(np.float32))\n    pid = str(p.get(\"id\") or p.get(\"uniq_id\") or i)\n    ids.append(pid)\n    meta = p.get(\"meta\", {}).copy()\n    meta.pop(\"local_image\", None)\n    metadata[pid] = meta\n\nmatrix = np.vstack(combined)\nprint(\"Combined matrix shape:\", matrix.shape)\nprint(\"Products without image (used text only):\", missing_image_count)\n\n# Save again (overwrite previous)\nimport json, os\nOUT_DIR = Path(\"/kaggle/working/index\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nindex_path = OUT_DIR / \"index.npz\"\nmeta_path = OUT_DIR / \"index.npz.meta.json\"\n\nnp.savez_compressed(str(index_path), ids=np.array(ids, dtype=object), matrix=matrix)\nwith open(meta_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved index to:\", index_path)\nprint(\"Saved metadata to:\", meta_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T03:53:34.169916Z","iopub.execute_input":"2025-10-18T03:53:34.170532Z","iopub.status.idle":"2025-10-18T03:54:16.953108Z","shell.execute_reply.started":"2025-10-18T03:53:34.170492Z","shell.execute_reply":"2025-10-18T03:54:16.951565Z"}},"outputs":[{"name":"stdout","text":"Tokenizer max length: 77\nPreparing chunks for 312 texts...\nTotal chunks produced: 572\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"chunk batches:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46dc1681dffb43bdbdccf42db829151d"}},"metadata":{}},{"name":"stdout","text":"Chunk embeddings shape: (572, 512)\nFinal text embeddings shape (per product): (312, 512)\nComputing image embeddings (images were not present in memory)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in <cell line: 0>:123                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m120 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m121 \u001b[0m\u001b[94mif\u001b[0m recompute_images:                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mComputing image embeddings (images were not present in memory)...\u001b[0m\u001b[33m\"\u001b[0m)             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m123 \u001b[2m│   \u001b[0mimg_emb = compute_image_embeddings(\u001b[1;4mimage_paths\u001b[0m, batch_size=\u001b[94m32\u001b[0m)                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m124 \u001b[0m\u001b[2m│   \u001b[0mimg_emb = l2_normalize_rows_np(img_emb)                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mImage embeddings shape:\u001b[0m\u001b[33m\"\u001b[0m, img_emb.shape)                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[94melse\u001b[0m:                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mNameError: \u001b[0mname \u001b[32m'image_paths'\u001b[0m is not defined\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in &lt;cell line: 0&gt;:123                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> recompute_images:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Computing image embeddings (images were not present in memory)...\"</span>)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>123 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>img_emb = compute_image_embeddings(<span style=\"font-weight: bold; text-decoration: underline\">image_paths</span>, batch_size=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">32</span>)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">124 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>img_emb = l2_normalize_rows_np(img_emb)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Image embeddings shape:\"</span>, img_emb.shape)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'image_paths'</span> is not defined\n</pre>\n"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Fix: define image_paths, compute image embeddings, recombine & save index\nfrom pathlib import Path\nimport numpy as np, json\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nimport torch\n\n# 1) ensure image_paths exists\nimage_paths = [p.get(\"local_image\") for p in products]\nprint(\"Example image paths (first 8):\", image_paths[:8])\nprint(\"Count image paths:\", len(image_paths))\n\n# 2) ensure compute_image_embeddings exists (define fallback safe version if not)\ntry:\n    compute_image_embeddings\nexcept NameError:\n    # define a safe compute_image_embeddings (same as earlier, but included here to be robust)\n    def compute_image_embeddings(paths, batch_size=16):\n        all_emb = []\n        with torch.no_grad():\n            for i in tqdm(range(0, len(paths), batch_size), desc=\"image batches\"):\n                batch_paths = paths[i:i+batch_size]\n                images = []\n                for p in batch_paths:\n                    if p is None:\n                        images.append(Image.new(\"RGB\",(224,224),color=(128,128,128)))\n                    else:\n                        try:\n                            img = Image.open(p).convert(\"RGB\")\n                        except Exception:\n                            img = Image.new(\"RGB\",(224,224),color=(128,128,128))\n                        images.append(img)\n                inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n                emb = model.get_image_features(**inputs)  # (B, D)\n                all_emb.append(emb.cpu().numpy())\n        if not all_emb:\n            return np.zeros((0, model.config.projection_dim), dtype=np.float32)\n        return np.vstack(all_emb).astype(np.float32)\n\n# 3) compute image embeddings (use smaller batch if memory issues)\nbatch_size_images = 16  # lower this to 8 if you get OOM\nprint(\"Computing image embeddings with batch_size =\", batch_size_images)\nimg_emb = compute_image_embeddings(image_paths, batch_size=batch_size_images)\n\n# 4) normalize image embeddings\ndef l2_normalize_rows_np(a):\n    if a is None or a.size == 0:\n        return a\n    norms = np.linalg.norm(a, axis=1, keepdims=True)\n    norms[norms==0] = 1.0\n    return a / norms\n\nimg_emb = l2_normalize_rows_np(img_emb)\nprint(\"Image embeddings shape:\", None if img_emb is None else img_emb.shape)\n\n# 5) Recombine with text_emb_new (must exist from previous chunking)\nif 'text_emb_new' not in globals():\n    raise RuntimeError(\"text_emb_new not found — rerun the chunked text embedding cell first.\")\ntext_emb = text_emb_new  # alias\n\n# weights\nw_text = 0.6\nw_img = 0.4\n\ncombined = []\nids = []\nmetadata = {}\nmissing_image_count = 0\ndim = text_emb.shape[1]\n\n# If img_emb has fewer rows than products, align by index (it should have same length)\nif img_emb.shape[0] != len(products):\n    print(\"Warning: img_emb rows != number of products. Filling missing entries with zeros.\")\n    # build img_emb_full with zeros then fill available rows\n    img_full = np.zeros((len(products), dim), dtype=np.float32)\n    n_fill = min(img_emb.shape[0], len(products))\n    img_full[:n_fill] = img_emb[:n_fill]\n    img_emb = img_full\n\nfor i, p in enumerate(products):\n    te = text_emb[i] if text_emb.shape[0] > i else np.zeros((dim,), dtype=np.float32)\n    ie = img_emb[i] if img_emb.shape[0] > i else np.zeros((dim,), dtype=np.float32)\n    if p.get(\"local_image\") is None or (ie is None or ie.size == 0):\n        v = te\n        missing_image_count += 1\n    else:\n        v = (w_text * te + w_img * ie)\n    v = v / (np.linalg.norm(v) + 1e-12)\n    combined.append(v.astype(np.float32))\n    pid = str(p.get(\"id\") or p.get(\"uniq_id\") or i)\n    ids.append(pid)\n    meta = p.get(\"meta\", {}).copy()\n    meta.pop(\"local_image\", None)\n    metadata[pid] = meta\n\nmatrix = np.vstack(combined)\nprint(\"Combined matrix shape:\", matrix.shape)\nprint(\"Products without image (used text only):\", missing_image_count)\n\n# 6) Save index & metadata\nOUT_DIR = Path(\"/kaggle/working/index\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nindex_path = OUT_DIR / \"index.npz\"\nmeta_path = OUT_DIR / \"index.npz.meta.json\"\n\nnp.savez_compressed(str(index_path), ids=np.array(ids, dtype=object), matrix=matrix)\nwith open(meta_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved index to:\", index_path)\nprint(\"Saved metadata to:\", meta_path)\n\n# 7) quick verification: load back and sample nearest neighbors for a sample prompt\ndata = np.load(str(index_path), allow_pickle=True)\nprint(\"Saved file entries:\", list(data.keys()))\nmat_loaded = data[\"matrix\"]\nprint(\"Loaded matrix shape:\", mat_loaded.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T04:00:47.325954Z","iopub.execute_input":"2025-10-18T04:00:47.328341Z","iopub.status.idle":"2025-10-18T04:01:15.796691Z","shell.execute_reply.started":"2025-10-18T04:00:47.328284Z","shell.execute_reply":"2025-10-18T04:01:15.795603Z"}},"outputs":[{"name":"stdout","text":"Example image paths (first 8): ['/kaggle/working/images/02593e81-5c09-5069-8516-b0b29f439ded.jpg', '/kaggle/working/images/5938d217-b8c5-5d3e-b1cf-e28e340f292e.jpg', '/kaggle/working/images/b2ede786-3f51-5a45-9a5b-bcf856958cd8.jpg', '/kaggle/working/images/8fd9377b-cfa6-5f10-835c-6b8eca2816b5.jpg', '/kaggle/working/images/bdc9aa30-9439-50dc-8e89-213ea211d66a.jpg', '/kaggle/working/images/20da3703-26f1-53e5-aa0b-a8104527d1bb.jpg', '/kaggle/working/images/aba4138e-6401-52ca-a099-02e30b638db4.jpg', '/kaggle/working/images/02593e81-5c09-5069-8516-b0b29f439ded.jpg']\nCount image paths: 312\nComputing image embeddings with batch_size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"image batches:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02860384d6fb418bb9fb77cf21bb2e9b"}},"metadata":{}},{"name":"stdout","text":"Image embeddings shape: (312, 512)\nCombined matrix shape: (312, 512)\nProducts without image (used text only): 0\nSaved index to: /kaggle/working/index/index.npz\nSaved metadata to: /kaggle/working/index/index.npz.meta.json\nSaved file entries: ['ids', 'matrix']\nLoaded matrix shape: (312, 512)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Evaluation utilities + load index & meta if not in memory\nimport numpy as np, json, math\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\n\nWORK_DIR = Path(\"/kaggle/working\")\nINDEX_DIR = WORK_DIR / \"index\"\nINDEX_PATH = INDEX_DIR / \"index.npz\"\nMETA_PATH = INDEX_DIR / \"index.npz.meta.json\"\n\n# try to reuse in-memory objects, otherwise load from disk\ntry:\n    ids  # check existence\n    print(\"Using in-memory 'ids' and 'matrix' variables\")\nexcept NameError:\n    print(\"Loading index from disk:\", INDEX_PATH)\n    data = np.load(str(INDEX_PATH), allow_pickle=True)\n    ids = data[\"ids\"].tolist()\n    matrix = data[\"matrix\"].astype(np.float32)\n\n# load metadata (if not in memory)\ntry:\n    metadata\nexcept NameError:\n    if META_PATH.exists():\n        with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n            metadata = json.load(f)\n        print(\"Loaded metadata entries:\", len(metadata))\n    else:\n        metadata = {}\n        print(\"No meta json found; metadata empty.\")\n\n# helper mapping id -> index\nid_to_index = {str(pid): idx for idx, pid in enumerate(ids)}\nn_items = len(ids)\nprint(\"n_items:\", n_items)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T04:06:01.855523Z","iopub.execute_input":"2025-10-18T04:06:01.856119Z","iopub.status.idle":"2025-10-18T04:06:01.867705Z","shell.execute_reply.started":"2025-10-18T04:06:01.856093Z","shell.execute_reply":"2025-10-18T04:06:01.866490Z"}},"outputs":[{"name":"stdout","text":"Using in-memory 'ids' and 'matrix' variables\nn_items: 312\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import re\n\n# Parse categories string into tokens\ndef parse_category_tokens(cat_val):\n    \"\"\"\n    Turn various category formats into a set of normalized tokens.\n    Handles strings like 'Electronics > Mice', \"'Home|Kitchen'\", \"['A','B']\" etc.\n    \"\"\"\n    if cat_val is None:\n        return set()\n    s = str(cat_val).strip()\n    if s == \"\" or s.lower() in (\"nan\",\"none\",\"[]\"):\n        return set()\n    # remove outer quotes if present\n    if (s.startswith('\"') and s.endswith('\"')) or (s.startswith(\"'\") and s.endswith(\"'\")):\n        s = s[1:-1].strip()\n    # try to replace JSON-like single quotes and parse lists\n    if s.startswith(\"[\") and s.endswith(\"]\"):\n        try:\n            import json\n            parsed = json.loads(s.replace(\"'\", '\"'))\n            if isinstance(parsed, (list, tuple)):\n                tokens = []\n                for it in parsed:\n                    if it:\n                        tokens.extend(re.split(r\"[>|/;,]\", str(it)))\n                tokens = [t.strip().lower() for t in tokens if t and t.strip()]\n                return set(tokens)\n        except Exception:\n            pass\n    # split on separators common in scraped data\n    parts = re.split(r\"[>|/;,|\\t\\n]\", s)\n    parts = [p.strip().lower() for p in parts if p and p.strip()]\n    return set(parts)\n\n# build tokens per item using metadata \"categories\" or fallback to brand/manufacturer\ncategory_tokens = {}\nfor pid in ids:\n    meta = metadata.get(str(pid), {})\n    # attempted keys\n    catraw = None\n    for k in (\"categories\",\"category\",\"cat\",\"product_categories\"):\n        if k in meta:\n            catraw = meta[k]\n            break\n    tokens = parse_category_tokens(catraw)\n    # fallback: brand or manufacturer token\n    if not tokens:\n        for k in (\"brand\",\"manufacturer\"):\n            if k in meta and meta[k]:\n                tokens = {str(meta[k]).strip().lower()}\n                break\n    category_tokens[str(pid)] = tokens\n\n# quick stats\ncounts = [len(t) for t in category_tokens.values()]\nimport numpy as np\nprint(\"category tokens per item: mean=%.2f median=%d (min=%d max=%d)\" % (np.mean(counts), int(np.median(counts)), int(np.min(counts)), int(np.max(counts))))\n# how many items with zero tokens\nzero_tokens = sum(1 for t in category_tokens.values() if len(t)==0)\nprint(\"Items with zero tokens (no categories/brand/manufacturer):\", zero_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T04:06:05.792785Z","iopub.execute_input":"2025-10-18T04:06:05.793211Z","iopub.status.idle":"2025-10-18T04:06:05.816908Z","shell.execute_reply.started":"2025-10-18T04:06:05.793161Z","shell.execute_reply":"2025-10-18T04:06:05.815518Z"}},"outputs":[{"name":"stdout","text":"category tokens per item: mean=4.44 median=4 (min=2 max=6)\nItems with zero tokens (no categories/brand/manufacturer): 0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# For each item, ground truth = other items that share >=1 token in category_tokens\ngt_sets = {}\nfor pid in ids:\n    toks = category_tokens.get(str(pid), set())\n    if not toks:\n        gt_sets[pid] = set()  # no ground truth available\n        continue\n    related = set()\n    # naive O(N) scan (OK for n~thousands; adapt to inverted index if larger)\n    for other in ids:\n        if other == pid:\n            continue\n        oth_toks = category_tokens.get(str(other), set())\n        if toks & oth_toks:\n            related.add(other)\n    gt_sets[pid] = related\n\n# Quick distribution of ground truth sizes\nsizes = [len(s) for s in gt_sets.values()]\nfrom collections import Counter\nctr = Counter(sizes)\nprint(\"GT size distribution (counts->num items):\", {k:v for k,v in sorted(ctr.items())[:10]})\nprint(\"Examples: product id, gt size (first 10):\")\nfor pid in list(ids)[:10]:\n    print(pid, len(gt_sets[pid]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T04:06:09.518735Z","iopub.execute_input":"2025-10-18T04:06:09.519120Z","iopub.status.idle":"2025-10-18T04:06:09.595586Z","shell.execute_reply.started":"2025-10-18T04:06:09.519088Z","shell.execute_reply":"2025-10-18T04:06:09.594269Z"}},"outputs":[{"name":"stdout","text":"GT size distribution (counts->num items): {1: 2, 5: 6, 12: 13, 16: 17, 20: 20, 24: 4, 35: 1, 191: 3, 214: 3, 235: 6}\nExamples: product id, gt size (first 10):\n02593e81-5c09-5069-8516-b0b29f439ded 238\n5938d217-b8c5-5d3e-b1cf-e28e340f292e 242\nb2ede786-3f51-5a45-9a5b-bcf856958cd8 20\n8fd9377b-cfa6-5f10-835c-6b8eca2816b5 20\nbdc9aa30-9439-50dc-8e89-213ea211d66a 241\n20da3703-26f1-53e5-aa0b-a8104527d1bb 241\naba4138e-6401-52ca-a099-02e30b638db4 241\n02593e81-5c09-5069-8516-b0b29f439ded 238\n5938d217-b8c5-5d3e-b1cf-e28e340f292e 242\nb2ede786-3f51-5a45-9a5b-bcf856958cd8 20\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# retrieval helpers: compute Recall@K and MRR for a matrix and ground truth sets\nimport numpy as np\n\ndef recall_at_k_for_all(query_matrix, target_matrix, topk_list=[1,5,10]):\n    \"\"\"\n    query_matrix: (N, D) numpy (one query per product) - rows normalized\n    target_matrix: (N, D) numpy (same index order as ids) - rows normalized\n    returns dict with recall@k and mrr\n    \"\"\"\n    N, D = target_matrix.shape\n    assert query_matrix.shape[0] == N\n    # compute similarity via dot product (rows normalized)\n    # sims matrix could be large O(N^2) but N=312 here is fine. For larger N, do batch.\n    sims = query_matrix @ target_matrix.T  # (N, N)\n    # set self-sim to -inf so we don't return itself as match\n    for i in range(N):\n        sims[i, i] = -999.0\n    results = {k: 0 for k in topk_list}\n    mrr_sum = 0.0\n    valid_queries = 0\n    for i, pid in enumerate(ids):\n        gt = gt_sets.get(pid, set())\n        if not gt:\n            # skip queries without ground truth\n            continue\n        valid_queries += 1\n        row = sims[i]\n        # get sorted indices desc\n        idxs = np.argsort(-row)\n        # compute ranks of the first hit\n        hit_rank = None\n        for rank, idx in enumerate(idxs, start=1):\n            if ids[int(idx)] in gt:\n                hit_rank = rank\n                break\n        if hit_rank is None:\n            # no hit in whole catalog\n            pass\n        else:\n            # update recalls\n            for k in topk_list:\n                if hit_rank <= k:\n                    results[k] += 1\n            mrr_sum += 1.0 / float(hit_rank)\n    # finalize metrics\n    if valid_queries == 0:\n        return {\"valid_queries\":0}\n    res = {}\n    for k in topk_list:\n        res[f\"recall@{k}\"] = results[k] / valid_queries\n    res[\"MRR\"] = mrr_sum / valid_queries\n    res[\"valid_queries\"] = valid_queries\n    return res\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T04:06:12.589403Z","iopub.execute_input":"2025-10-18T04:06:12.589775Z","iopub.status.idle":"2025-10-18T04:06:12.601527Z","shell.execute_reply.started":"2025-10-18T04:06:12.589743Z","shell.execute_reply":"2025-10-18T04:06:12.600081Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Prepare matrices: try to reuse in-memory text_emb_new, img_emb; otherwise recompute or fallback\n# 'matrix' variable should be the combined matrix we saved earlier (rows normalized)\ntry:\n    combined_matrix = matrix  # from disk or earlier\nexcept NameError:\n    combined_matrix = np.load(str(INDEX_PATH), allow_pickle=True)[\"matrix\"].astype(np.float32)\n\n# ensure normalized\ndef normalize_rows(a):\n    norms = np.linalg.norm(a, axis=1, keepdims=True)\n    norms[norms==0] = 1.0\n    return a / norms\n\ncombined_matrix = normalize_rows(combined_matrix)\n\n# text-only\ntry:\n    text_matrix = text_emb_new  # computed by the chunking step earlier\n    print(\"Using in-memory text_emb_new with shape\", text_matrix.shape)\nexcept NameError:\n    # attempt to compute quick text-only by encoding titles only (faster but less accurate)\n    print(\"text_emb_new not found in memory. Recomputing text-only using titles+desc chunking...\")\n    # shortened re-run of chunking -> compute embeddings (could take time)\n    raise RuntimeError(\"text_emb_new missing — rerun chunked embedding cell or let me compute text-only now.\")\n\ntext_matrix = normalize_rows(text_matrix)\n\n# image-only\ntry:\n    img_matrix = img_emb  # from earlier compute_image_embeddings\n    print(\"Using in-memory img_emb with shape\", img_matrix.shape)\nexcept NameError:\n    # If image-only not available, fallback to zeros\n    print(\"img_emb missing; building zero-image matrix\")\n    img_matrix = np.zeros_like(text_matrix)\n\nimg_matrix = normalize_rows(img_matrix)\n\n# compute metrics (Recall@1,5,10 and MRR)\ntopk = [1,5,10]\nprint(\"Evaluating combined...\")\ncombined_res = recall_at_k_for_all(combined_matrix, combined_matrix, topk_list=topk)\nprint(\"Combined:\", combined_res)\n\nprint(\"Evaluating text-only...\")\ntext_res = recall_at_k_for_all(text_matrix, text_matrix, topk_list=topk)\nprint(\"Text-only:\", text_res)\n\nprint(\"Evaluating image-only...\")\nimg_res = recall_at_k_for_all(img_matrix, img_matrix, topk_list=topk)\nprint(\"Image-only:\", img_res)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T04:06:16.628281Z","iopub.execute_input":"2025-10-18T04:06:16.628619Z","iopub.status.idle":"2025-10-18T04:06:16.679457Z","shell.execute_reply.started":"2025-10-18T04:06:16.628598Z","shell.execute_reply":"2025-10-18T04:06:16.678102Z"}},"outputs":[{"name":"stdout","text":"Using in-memory text_emb_new with shape (312, 512)\nUsing in-memory img_emb with shape (312, 512)\nEvaluating combined...\nCombined: {'recall@1': 0.8621794871794872, 'recall@5': 0.9807692307692307, 'recall@10': 0.9871794871794872, 'MRR': 0.9166361326608706, 'valid_queries': 312}\nEvaluating text-only...\nText-only: {'recall@1': 0.8525641025641025, 'recall@5': 0.9711538461538461, 'recall@10': 0.9839743589743589, 'MRR': 0.9088039809857403, 'valid_queries': 312}\nEvaluating image-only...\nImage-only: {'recall@1': 0.8237179487179487, 'recall@5': 0.9583333333333334, 'recall@10': 0.9743589743589743, 'MRR': 0.8854238907027263, 'valid_queries': 312}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Compute per-category (top token) average Recall@5 to see which categories perform well\nfrom collections import defaultdict\ncat_to_pids = defaultdict(list)\nfor pid, toks in category_tokens.items():\n    if not toks:\n        continue\n    # pick primary token as first sorted token (heuristic)\n    primary = sorted(toks)[0]\n    cat_to_pids[primary].append(pid)\n\ndef recall_at_k_for_pidlist(pid_list, matrix, k=5):\n    # compute recall for this set of queries only\n    if not pid_list:\n        return None\n    sub_q_idx = [id_to_index[pid] for pid in pid_list if pid in id_to_index]\n    if not sub_q_idx:\n        return None\n    # build sub query matrix and reuse full matrix as target\n    qmat = matrix[sub_q_idx]\n    res = recall_at_k_for_all(qmat, matrix, topk_list=[k])\n    return res\n\n# evaluate top categories with >= 5 items\ncat_scores = []\nfor cat, pids in cat_to_pids.items():\n    if len(pids) < 5:\n        continue\n    res = recall_at_k_for_pidlist(pids, combined_matrix, k=5)\n    if res and res.get(\"valid_queries\",0)>0:\n        cat_scores.append((cat, len(pids), res[\"recall@5\"]))\ncat_scores_sorted = sorted(cat_scores, key=lambda x: -x[2])\nprint(\"Top categories by recall@5 (combined):\")\nfor cat, cnt, score in cat_scores_sorted[:15]:\n    print(f\"{cat}: items={cnt}, recall@5={score:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T04:06:21.014801Z","iopub.execute_input":"2025-10-18T04:06:21.015214Z","iopub.status.idle":"2025-10-18T04:06:21.188129Z","shell.execute_reply.started":"2025-10-18T04:06:21.015161Z","shell.execute_reply":"2025-10-18T04:06:21.186475Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in <cell line: 0>:28                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[94mfor\u001b[0m cat, pids \u001b[95min\u001b[0m cat_to_pids.items():                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(pids) < \u001b[94m5\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mcontinue\u001b[0m                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m28 \u001b[2m│   \u001b[0mres = \u001b[1;4mrecall_at_k_for_pidlist(pids, combined_matrix, k=\u001b[0m\u001b[1;4;94m5\u001b[0m\u001b[1;4m)\u001b[0m                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m res \u001b[95mand\u001b[0m res.get(\u001b[33m\"\u001b[0m\u001b[33mvalid_queries\u001b[0m\u001b[33m\"\u001b[0m,\u001b[94m0\u001b[0m)>\u001b[94m0\u001b[0m:                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m│   │   \u001b[0mcat_scores.append((cat, \u001b[96mlen\u001b[0m(pids), res[\u001b[33m\"\u001b[0m\u001b[33mrecall@5\u001b[0m\u001b[33m\"\u001b[0m]))                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0mcat_scores_sorted = \u001b[96msorted\u001b[0m(cat_scores, key=\u001b[94mlambda\u001b[0m x: -x[\u001b[94m2\u001b[0m])                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in recall_at_k_for_pidlist:20                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[94mNone\u001b[0m                                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# build sub query matrix and reuse full matrix as target\u001b[0m                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0mqmat = matrix[sub_q_idx]                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m20 \u001b[2m│   \u001b[0mres = \u001b[1;4mrecall_at_k_for_all(qmat, matrix, topk_list=[k])\u001b[0m                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m res                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m\u001b[2m# evaluate top categories with >= 5 items\u001b[0m                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in recall_at_k_for_all:11                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mreturns dict with recall@k and mrr\u001b[0m                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   \u001b[0mN, D = target_matrix.shape                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m11 \u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m \u001b[1;4mquery_matrix.shape[\u001b[0m\u001b[1;4;94m0\u001b[0m\u001b[1;4m] == N\u001b[0m                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# compute similarity via dot product (rows normalized)\u001b[0m                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# sims matrix could be large O(N^2) but N=312 here is fine. For larger N, do batch.\u001b[0m     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   \u001b[0msims = query_matrix @ target_matrix.T  \u001b[2m# (N, N)\u001b[0m                                         \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mAssertionError\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in &lt;cell line: 0&gt;:28                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> cat, pids <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> cat_to_pids.items():                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(pids) &lt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">continue</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>28 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>res = <span style=\"font-weight: bold; text-decoration: underline\">recall_at_k_for_pidlist(pids, combined_matrix, k=</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">5</span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> res <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> res.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"valid_queries\"</span>,<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)&gt;<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>cat_scores.append((cat, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(pids), res[<span style=\"color: #808000; text-decoration-color: #808000\">\"recall@5\"</span>]))                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span>cat_scores_sorted = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">sorted</span>(cat_scores, key=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">lambda</span> x: -x[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>])                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in recall_at_k_for_pidlist:20                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># build sub query matrix and reuse full matrix as target</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>qmat = matrix[sub_q_idx]                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>20 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>res = <span style=\"font-weight: bold; text-decoration: underline\">recall_at_k_for_all(qmat, matrix, topk_list=[k])</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> res                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 # evaluate top categories with &gt;= 5 items</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in recall_at_k_for_all:11                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">returns dict with recall@k and mrr</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>N, D = target_matrix.shape                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>11 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"font-weight: bold; text-decoration: underline\">query_matrix.shape[</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">0</span><span style=\"font-weight: bold; text-decoration: underline\">] == N</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># compute similarity via dot product (rows normalized)</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># sims matrix could be large O(N^2) but N=312 here is fine. For larger N, do batch.</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>sims = query_matrix @ target_matrix.T  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (N, N)</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AssertionError</span>\n</pre>\n"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# --- Fixed evaluation cell: supports query_matrix with different size than target_matrix ---\nimport numpy as np\nfrom collections import defaultdict\n\n# fixed recall/MRR function: query_matrix can be MxD, target_matrix NxD\ndef recall_and_mrr(query_matrix: np.ndarray, target_matrix: np.ndarray, ids_list, topk_list=[1,5,10], gt_sets_local=None):\n    \"\"\"\n    query_matrix: (M, D)\n    target_matrix: (N, D)\n    ids_list: list of length N containing ids matched to rows of target_matrix\n    gt_sets_local: dict mapping id -> set(of relevant ids). If None, function will return None.\n    Returns dict of metrics and number of valid queries.\n    \"\"\"\n    M, D = query_matrix.shape\n    N, D2 = target_matrix.shape\n    assert D == D2, \"dimension mismatch\"\n    if gt_sets_local is None:\n        raise ValueError(\"gt_sets_local (ground truth dict) is required\")\n\n    sims_block = query_matrix @ target_matrix.T  # shape (M, N)\n    # For each query we should avoid returning itself as top result if the id exists in ids_list.\n    # But query_matrix may be a subset; we need mapping from query index -> query id.\n    # We'll assume queries are in the same order as some provided list 'query_ids' when calling from outside.\n    # To be flexible, we'll accept that caller will provide 'query_ids_for_matrix' if needed.\n    results = {k: 0 for k in topk_list}\n    mrr_sum = 0.0\n    valid_queries = 0\n\n    # We'll expect the caller to pass query_ids_for_matrix (list length M) to identify self-hits\n    # To keep usage simple here, we'll build an index map from id->row index in target_matrix\n    id_to_idx = {str(pid): idx for idx, pid in enumerate(ids_list)}\n\n    for qi in range(M):\n        qid = query_ids_for_matrix[qi]  # must be defined in caller scope\n        gt = gt_sets_local.get(qid, set())\n        if not gt:\n            continue\n        valid_queries += 1\n        row = sims_block[qi].copy()\n        # if qid exists in target_matrix ids, mask its self-similarity\n        if qid in id_to_idx:\n            row[id_to_idx[qid]] = -999.0\n        idxs = np.argsort(-row)  # indices into target_matrix\n        hit_rank = None\n        for rank, idx in enumerate(idxs, start=1):\n            cand_id = ids_list[int(idx)]\n            if cand_id in gt:\n                hit_rank = rank\n                break\n        if hit_rank is not None:\n            for k in topk_list:\n                if hit_rank <= k:\n                    results[k] += 1\n            mrr_sum += 1.0 / float(hit_rank)\n    if valid_queries == 0:\n        out = {f\"recall@{k}\": None for k in topk_list}\n        out[\"MRR\"] = None\n        out[\"valid_queries\"] = 0\n        return out\n    out = {}\n    for k in topk_list:\n        out[f\"recall@{k}\"] = results[k] / valid_queries\n    out[\"MRR\"] = mrr_sum / valid_queries\n    out[\"valid_queries\"] = valid_queries\n    return out\n\n# --- Prepare matrices and ids ---\n# combined_matrix, text_matrix, img_matrix, ids (target ids) and gt_sets must already exist in memory\n# If not, load them:\ntry:\n    combined_matrix\nexcept NameError:\n    combined_matrix = np.load(str(INDEX_PATH), allow_pickle=True)[\"matrix\"].astype(np.float32)\ntry:\n    ids\nexcept NameError:\n    ids = np.load(str(INDEX_PATH), allow_pickle=True)[\"ids\"].tolist()\ntry:\n    text_matrix\nexcept NameError:\n    raise RuntimeError(\"text_matrix (text_emb_new) missing — rerun the chunked text embedding cell first.\")\ntry:\n    img_matrix\nexcept NameError:\n    img_matrix = np.zeros_like(text_matrix)\n\n# normalize all\ndef normalize_rows(a):\n    norms = np.linalg.norm(a, axis=1, keepdims=True)\n    norms[norms==0] = 1.0\n    return a / norms\n\ncombined_matrix = normalize_rows(combined_matrix)\ntext_matrix = normalize_rows(text_matrix)\nimg_matrix = normalize_rows(img_matrix)\n\n# --- Evaluate on full-catalog queries (M = N) ---\n# for full-catalog evaluation, query_ids_for_matrix is the same as ids\nquery_ids_for_matrix = ids  # required by recall_and_mrr\nprint(\"Evaluating full-catalog (each product queries the catalog)...\")\ncombined_res = recall_and_mrr(combined_matrix, combined_matrix, ids, topk_list=[1,5,10], gt_sets_local=gt_sets)\nprint(\"Combined:\", combined_res)\ntext_res = recall_and_mrr(text_matrix, combined_matrix, ids, topk_list=[1,5,10], gt_sets_local=gt_sets)\nprint(\"Text-only:\", text_res)\nimg_res = recall_and_mrr(img_matrix, combined_matrix, ids, topk_list=[1,5,10], gt_sets_local=gt_sets)\nprint(\"Image-only:\", img_res)\n\n# --- Per-category breakdown using subset queries ---\n# Build cat->pid mapping (category_tokens from earlier cell)\ncat_to_pids = defaultdict(list)\nfor pid, toks in category_tokens.items():\n    if not toks:\n        continue\n    primary = sorted(toks)[0]\n    cat_to_pids[primary].append(pid)\n\ncat_scores = []\nfor cat, pids in cat_to_pids.items():\n    if len(pids) < 5:\n        continue\n    # build query matrix (M x D) from indices of these pids using combined_matrix rows\n    q_idxs = [id_to_index[pid] for pid in pids if pid in id_to_index]\n    if not q_idxs:\n        continue\n    qmat = combined_matrix[q_idxs]\n    # define query ids list for this qmat (must set global name query_ids_for_matrix used by function)\n    query_ids_for_matrix = [ids[i] for i in q_idxs]\n    res = recall_and_mrr(qmat, combined_matrix, ids, topk_list=[5], gt_sets_local=gt_sets)\n    if res and res.get(\"valid_queries\",0) > 0:\n        cat_scores.append((cat, len(pids), res[\"recall@5\"]))\n# sort and print\ncat_scores_sorted = sorted(cat_scores, key=lambda x: -x[2])\nprint(\"Top categories by recall@5 (combined):\")\nfor cat, cnt, score in cat_scores_sorted[:15]:\n    print(f\"{cat}: items={cnt}, recall@5={score:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T04:06:25.359564Z","iopub.execute_input":"2025-10-18T04:06:25.360541Z","iopub.status.idle":"2025-10-18T04:06:25.423482Z","shell.execute_reply.started":"2025-10-18T04:06:25.360508Z","shell.execute_reply":"2025-10-18T04:06:25.422518Z"}},"outputs":[{"name":"stdout","text":"Evaluating full-catalog (each product queries the catalog)...\nCombined: {'recall@1': 0.8621794871794872, 'recall@5': 0.9807692307692307, 'recall@10': 0.9871794871794872, 'MRR': 0.9166361326608706, 'valid_queries': 312}\nText-only: {'recall@1': 0.8621794871794872, 'recall@5': 0.9743589743589743, 'recall@10': 0.9903846153846154, 'MRR': 0.9139217610331918, 'valid_queries': 312}\nImage-only: {'recall@1': 0.8557692307692307, 'recall@5': 0.9583333333333334, 'recall@10': 0.9743589743589743, 'MRR': 0.9053998558998558, 'valid_queries': 312}\nTop categories by recall@5 (combined):\nclothing & closet storage: items=11, recall@5=1.000\nchairs: items=15, recall@5=1.000\ndoormats: items=20, recall@5=1.000\nfurniture: items=68, recall@5=1.000\nbathroom furniture: items=5, recall@5=1.000\nbedroom furniture: items=13, recall@5=1.000\nbathroom hardware: items=17, recall@5=1.000\naccessories: items=5, recall@5=1.000\nend tables: items=21, recall@5=1.000\nbaby products: items=6, recall@5=1.000\ndining room furniture: items=5, recall@5=1.000\nhome & kitchen: items=29, recall@5=1.000\nbarstools: items=18, recall@5=1.000\n\"kids' furniture\": items=13, recall@5=0.846\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Build text list and image path list from `products`.\n# products must exist from earlier cells and have fields 'title','description','id','local_image','meta'\ntexts = [(p.get(\"title\",\"\") + \". \" + (p.get(\"description\") or \"\")) for p in products]\nimage_paths = [p.get(\"local_image\") for p in products]\n\nprint(\"Number of products:\", len(products))\nprint(\"Number of text items:\", len(texts))\nprint(\"Number of image paths:\", len(image_paths))\nprint(\"Example image path (first 5):\", image_paths[:5])\n\n# Compute text embeddings\ntext_emb = compute_text_embeddings(texts, batch_size=64)\nprint(\"Text embeddings shape:\", text_emb.shape)\ntext_emb = l2_normalize_rows(text_emb)\n\n# Compute image embeddings\nimg_emb = compute_image_embeddings(image_paths, batch_size=32)\nprint(\"Image embeddings shape:\", img_emb.shape)\nimg_emb = l2_normalize_rows(img_emb)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:36:00.226166Z","iopub.execute_input":"2025-10-17T18:36:00.226669Z","iopub.status.idle":"2025-10-17T18:36:01.116062Z","shell.execute_reply.started":"2025-10-17T18:36:00.226636Z","shell.execute_reply":"2025-10-17T18:36:01.114441Z"}},"outputs":[{"name":"stdout","text":"Number of products: 312\nNumber of text items: 312\nNumber of image paths: 312\nExample image path (first 5): ['/kaggle/working/images/02593e81-5c09-5069-8516-b0b29f439ded.jpg', '/kaggle/working/images/5938d217-b8c5-5d3e-b1cf-e28e340f292e.jpg', '/kaggle/working/images/b2ede786-3f51-5a45-9a5b-bcf856958cd8.jpg', '/kaggle/working/images/8fd9377b-cfa6-5f10-835c-6b8eca2816b5.jpg', '/kaggle/working/images/bdc9aa30-9439-50dc-8e89-213ea211d66a.jpg']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"text batches:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ebb8a2b88464225b6628bc8f1a6e180"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (379 > 77). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in <cell line: 0>:12                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mExample image path (first 5):\u001b[0m\u001b[33m\"\u001b[0m, image_paths[:\u001b[94m5\u001b[0m])                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m# Compute text embeddings\u001b[0m                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m12 text_emb = \u001b[1;4mcompute_text_embeddings(texts, batch_size=\u001b[0m\u001b[1;4;94m64\u001b[0m\u001b[1;4m)\u001b[0m                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mText embeddings shape:\u001b[0m\u001b[33m\"\u001b[0m, text_emb.shape)                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0mtext_emb = l2_normalize_rows(text_emb)                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in compute_text_embeddings:32                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m tqdm(\u001b[96mrange\u001b[0m(\u001b[94m0\u001b[0m, \u001b[96mlen\u001b[0m(texts), batch_size), desc=\u001b[33m\"\u001b[0m\u001b[33mtext batches\u001b[0m\u001b[33m\"\u001b[0m):               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatch = texts[i:i+batch_size]                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs = processor(text=batch, return_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m, padding=\u001b[94mTrue\u001b[0m).to(device)    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m32 \u001b[2m│   │   │   \u001b[0memb = \u001b[1;4mmodel.get_text_features(**inputs)\u001b[0m   \u001b[2m# (B, D)\u001b[0m                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m33 \u001b[0m\u001b[2m│   │   │   \u001b[0mall_emb.append(emb.cpu().numpy())                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m34 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m all_emb:                                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m np.zeros((\u001b[94m0\u001b[0m, model.config.projection_dim), dtype=np.float32)                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/\u001b[0m\u001b[1mmodeling_clip.py\u001b[0m:1004 in        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m get_text_features                                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1001 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1002 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1003 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1004 \u001b[2m│   │   \u001b[0mtext_outputs = \u001b[1;4;96mself\u001b[0m\u001b[1;4m.text_model(\u001b[0m                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1005 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4minput_ids=input_ids,\u001b[0m                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1006 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4mattention_mask=attention_mask,\u001b[0m                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1007 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4mposition_ids=position_ids,\u001b[0m                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1mmodule.py\u001b[0m:1739 in _wrapped_call_impl    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1736 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1737 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type: ignore[misc]\u001b[0m        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1738 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1739 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._call_impl(*args, **kwargs)\u001b[0m                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1740 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1741 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# torchrec tests the code consistency with the following code\u001b[0m                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1742 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# fmt: off\u001b[0m                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1mmodule.py\u001b[0m:1750 in _call_impl            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1747 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1748 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1749 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1750 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mforward_call(*args, **kwargs)\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1751 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1752 \u001b[0m\u001b[2m│   │   \u001b[0mresult = \u001b[94mNone\u001b[0m                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1753 \u001b[0m\u001b[2m│   │   \u001b[0mcalled_always_called_hooks = \u001b[96mset\u001b[0m()                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/\u001b[0m\u001b[1mmodeling_clip.py\u001b[0m:699 in forward \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 696 \u001b[0m\u001b[2m│   │   \u001b[0minput_shape = input_ids.size()                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 697 \u001b[0m\u001b[2m│   │   \u001b[0minput_ids = input_ids.view(-\u001b[94m1\u001b[0m, input_shape[-\u001b[94m1\u001b[0m])                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 698 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 699 \u001b[2m│   │   \u001b[0mhidden_states = \u001b[1;4;96mself\u001b[0m\u001b[1;4m.embeddings(input_ids=input_ids, position_ids=position_ids)\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 700 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 701 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# CLIP's text model uses causal mask, prepare it here.\u001b[0m                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 702 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/c\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1mmodule.py\u001b[0m:1739 in _wrapped_call_impl    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1736 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1737 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type: ignore[misc]\u001b[0m        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1738 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1739 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._call_impl(*args, **kwargs)\u001b[0m                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1740 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1741 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# torchrec tests the code consistency with the following code\u001b[0m                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1742 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# fmt: off\u001b[0m                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1mmodule.py\u001b[0m:1750 in _call_impl            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1747 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1748 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1749 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1750 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mforward_call(*args, **kwargs)\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1751 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1752 \u001b[0m\u001b[2m│   │   \u001b[0mresult = \u001b[94mNone\u001b[0m                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1753 \u001b[0m\u001b[2m│   │   \u001b[0mcalled_always_called_hooks = \u001b[96mset\u001b[0m()                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/\u001b[0m\u001b[1mmodeling_clip.py\u001b[0m:222 in forward \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 219 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs_embeds = \u001b[96mself\u001b[0m.token_embedding(input_ids)                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 220 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 221 \u001b[0m\u001b[2m│   │   \u001b[0mposition_embeddings = \u001b[96mself\u001b[0m.position_embedding(position_ids)                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 222 \u001b[2m│   │   \u001b[0membeddings = \u001b[1;4minputs_embeds + position_embeddings\u001b[0m                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 223 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 224 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m embeddings                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 225 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mRuntimeError: \u001b[0mThe size of tensor a \u001b[1m(\u001b[0m\u001b[1;36m379\u001b[0m\u001b[1m)\u001b[0m must match the size of tensor b \u001b[1m(\u001b[0m\u001b[1;36m77\u001b[0m\u001b[1m)\u001b[0m at non-singleton dimension \u001b[1;36m1\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in &lt;cell line: 0&gt;:12                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Example image path (first 5):\"</span>, image_paths[:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>])                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 # Compute text embeddings</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>12 text_emb = <span style=\"font-weight: bold; text-decoration: underline\">compute_text_embeddings(texts, batch_size=</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">64</span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Text embeddings shape:\"</span>, text_emb.shape)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span>text_emb = l2_normalize_rows(text_emb)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in compute_text_embeddings:32                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> tqdm(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(texts), batch_size), desc=<span style=\"color: #808000; text-decoration-color: #808000\">\"text batches\"</span>):               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>batch = texts[i:i+batch_size]                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>inputs = processor(text=batch, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>, padding=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>).to(device)    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>32 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>emb = <span style=\"font-weight: bold; text-decoration: underline\">model.get_text_features(**inputs)</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (B, D)</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">33 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>all_emb.append(emb.cpu().numpy())                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">34 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> all_emb:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> np.zeros((<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, model.config.projection_dim), dtype=np.float32)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/transformers/models/clip/</span><span style=\"font-weight: bold\">modeling_clip.py</span>:1004 in        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> get_text_features                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1001 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1002 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1003 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1004 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>text_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.text_model(</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1005 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">input_ids=input_ids,</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1006 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">attention_mask=attention_mask,</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1007 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">position_ids=position_ids,</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/torch/nn/modules/</span><span style=\"font-weight: bold\">module.py</span>:1739 in _wrapped_call_impl    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1736 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1737 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[misc]</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1738 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1739 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">._call_impl(*args, **kwargs)</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1741 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># torchrec tests the code consistency with the following code</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1742 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># fmt: off</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/torch/nn/modules/</span><span style=\"font-weight: bold\">module.py</span>:1750 in _call_impl            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1747 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1748 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1749 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1750 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"font-weight: bold; text-decoration: underline\">forward_call(*args, **kwargs)</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1751 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1752 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1753 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>called_always_called_hooks = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">set</span>()                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/transformers/models/clip/</span><span style=\"font-weight: bold\">modeling_clip.py</span>:699 in forward <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 696 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>input_shape = input_ids.size()                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 697 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>input_ids = input_ids.view(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, input_shape[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>])                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 698 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 699 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.embeddings(input_ids=input_ids, position_ids=position_ids)</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 700 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 701 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># CLIP's text model uses causal mask, prepare it here.</span>                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 702 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/c</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/torch/nn/modules/</span><span style=\"font-weight: bold\">module.py</span>:1739 in _wrapped_call_impl    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1736 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1737 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[misc]</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1738 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1739 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">._call_impl(*args, **kwargs)</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1741 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># torchrec tests the code consistency with the following code</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1742 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># fmt: off</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/torch/nn/modules/</span><span style=\"font-weight: bold\">module.py</span>:1750 in _call_impl            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1747 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1748 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1749 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1750 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"font-weight: bold; text-decoration: underline\">forward_call(*args, **kwargs)</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1751 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1752 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1753 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>called_always_called_hooks = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">set</span>()                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/transformers/models/clip/</span><span style=\"font-weight: bold\">modeling_clip.py</span>:222 in forward <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 219 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>inputs_embeds = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.token_embedding(input_ids)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 220 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 221 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>position_embeddings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.position_embedding(position_ids)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 222 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>embeddings = <span style=\"font-weight: bold; text-decoration: underline\">inputs_embeds + position_embeddings</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 223 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 224 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> embeddings                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 225 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>The size of tensor a <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">379</span><span style=\"font-weight: bold\">)</span> must match the size of tensor b <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span><span style=\"font-weight: bold\">)</span> at non-singleton dimension <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n</pre>\n"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nWORK_DIR = Path(\"/kaggle/working\")\nOUT_DIR = WORK_DIR / \"index\"\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nindex_path = OUT_DIR / \"index.npz\"\nmeta_path = OUT_DIR / \"index.npz.meta.json\"\n\n# combination strategy: weighted avg. change these weights if desired.\nw_text = 0.6\nw_img = 0.4\n\ncombined = []\nids = []\nmetadata = {}\nmissing_image_count = 0\n\nfor i, p in enumerate(products):\n    te = text_emb[i] if text_emb.shape[0] > i else np.zeros((model.config.projection_dim,), dtype=np.float32)\n    ie = img_emb[i] if img_emb.shape[0] > i else np.zeros((model.config.projection_dim,), dtype=np.float32)\n    if p.get(\"local_image\") is None or (ie is None or ie.size == 0):\n        v = te\n        missing_image_count += 1\n    else:\n        v = (w_text * te + w_img * ie)\n    # normalize\n    v = v / (np.linalg.norm(v) + 1e-12)\n    combined.append(v.astype(np.float32))\n    pid = str(p.get(\"id\") or p.get(\"uniq_id\") or i)\n    ids.append(pid)\n    # metadata: keep provided meta dict (drop local_image to keep payload small)\n    meta = p.get(\"meta\", {}).copy()\n    meta.pop(\"local_image\", None)\n    metadata[pid] = meta\n\nmatrix = np.vstack(combined)\nprint(\"Combined matrix shape:\", matrix.shape)\nprint(\"Products without image (used text only):\", missing_image_count)\n\n# Save .npz and meta.json\nnp.savez_compressed(str(index_path), ids=np.array(ids, dtype=object), matrix=matrix)\nwith open(meta_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved index to:\", index_path)\nprint(\"Saved metadata to:\", meta_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T04:06:59.897597Z","iopub.execute_input":"2025-10-18T04:06:59.897952Z","iopub.status.idle":"2025-10-18T04:06:59.970296Z","shell.execute_reply.started":"2025-10-18T04:06:59.897930Z","shell.execute_reply":"2025-10-18T04:06:59.969216Z"}},"outputs":[{"name":"stdout","text":"Combined matrix shape: (312, 512)\nProducts without image (used text only): 0\nSaved index to: /kaggle/working/index/index.npz\nSaved metadata to: /kaggle/working/index/index.npz.meta.json\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# quick verify by loading back and doing a sanity nearest neighbor for a sample prompt\nimport numpy as np\ndata = np.load(str(index_path), allow_pickle=True)\nids_loaded = data[\"ids\"].tolist()\nmat_loaded = data[\"matrix\"]\nprint(\"Loaded ids:\", len(ids_loaded))\nprint(\"Loaded matrix shape:\", mat_loaded.shape)\n\n# Example: compute embedding for sample prompt & find top-5 nearest (cosine)\nsample_prompt = \"cat\"\n# compute text embedding using CLIP processor + model\ninputs = processor(text=[sample_prompt], return_tensors=\"pt\", padding=True).to(device)\nwith torch.no_grad():\n    q = model.get_text_features(**inputs).cpu().numpy()[0]\n# normalize\nq = q / (np.linalg.norm(q) + 1e-12)\n\n# cosine via dot (matrix rows are normalized)\nsims = (mat_loaded @ q).reshape(-1)\ntopk = 5\nidxs = np.argsort(-sims)[:topk]\nprint(\"Top results (id,score):\")\nfor ii in idxs:\n    pid = ids_loaded[int(ii)]\n    score = float(sims[int(ii)])\n    print(pid, score)\n    # print title from metadata for convenience\n    try:\n        print(\"  title:\", metadata[pid].get(\"title\")[:150])\n    except Exception:\n        pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T04:08:01.097150Z","iopub.execute_input":"2025-10-18T04:08:01.097559Z","iopub.status.idle":"2025-10-18T04:08:01.171939Z","shell.execute_reply.started":"2025-10-18T04:08:01.097532Z","shell.execute_reply":"2025-10-18T04:08:01.170812Z"}},"outputs":[{"name":"stdout","text":"Loaded ids: 312\nLoaded matrix shape: (312, 512)\nTop results (id,score):\n2631f7c4-fc61-5b16-9824-7acf7fd59994 0.6638836860656738\n  title: Simple Deluxe Gaming Chair, Big and Tall Gamer Chair, Racing Style Adjustable Swivel Office Chair, Ergonomic Video Game Chairs with Headrest and Lumba\n780dda1f-e7ef-598e-9c0e-6536c4b4c261 0.660207986831665\n  title: Stylish Camping Ming's Mark RC4 Reversible Classical Patio Mat - 8' x 20', Green/Beige\n37323128-75a9-578b-9378-79653bfd5b52 0.649467945098877\n  title: FANYE Oversized 6 Seaters Modular Storage Sectional Sofa Couch for Home Apartment Office Living Room,Free Combination L/U Shaped Corduroy Upholstered \n20da3703-26f1-53e5-aa0b-a8104527d1bb 0.6392088532447815\n  title: LOVMOR 30'' Bathroom Vanity Sink Base Cabine, Storage Cabinet with 3-Drawers on The Left, Suitable for Bathrooms, Kitchens, Laundry Rooms and Other Pl\n20da3703-26f1-53e5-aa0b-a8104527d1bb 0.6392088532447815\n  title: LOVMOR 30'' Bathroom Vanity Sink Base Cabine, Storage Cabinet with 3-Drawers on The Left, Suitable for Bathrooms, Kitchens, Laundry Rooms and Other Pl\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"meow","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)\nmodel_name = \"openai/clip-vit-base-patch32\"\nmodel = CLIPModel.from_pretrained(model_name).to(device)\nprocessor = CLIPProcessor.from_pretrained(model_name)\nmodel.eval()\n\ndef compute_text_embeddings(texts, batch_size=64):\n    all_emb = []\n    with torch.no_grad():\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i+batch_size]\n            inputs = processor(text=batch, return_tensors=\"pt\", padding=True).to(device)\n            emb = model.get_text_features(**inputs)\n            all_emb.append(emb.cpu().numpy())\n    if all_emb:\n        return np.vstack(all_emb)\n    return np.zeros((0, model.config.projection_dim), dtype=np.float32)\n\ndef compute_image_embeddings(paths, batch_size=32):\n    all_emb = []\n    with torch.no_grad():\n        for i in range(0, len(paths), batch_size):\n            batch = paths[i:i+batch_size]\n            images = []\n            for p in batch:\n                if p is None:\n                    images.append(Image.new(\"RGB\",(224,224),color=(128,128,128)))\n                else:\n                    try:\n                        img = Image.open(p).convert(\"RGB\")\n                    except Exception:\n                        img = Image.new(\"RGB\",(224,224),color=(128,128,128))\n                    images.append(img)\n            inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n            emb = model.get_image_features(**inputs)\n            all_emb.append(emb.cpu().numpy())\n    if all_emb:\n        return np.vstack(all_emb)\n    return np.zeros((0, model.config.projection_dim), dtype=np.float32)\n\n# Prepare lists\ntexts = [ (p[\"title\"] + \". \" + (p[\"description\"] or \"\")) for p in products ]\nimage_paths = [ p.get(\"local_image\") for p in products ]\n\nprint(\"Computing text embeddings...\")\ntext_emb = compute_text_embeddings(texts, batch_size=64)\nprint(\"Computing image embeddings...\")\nimg_emb = compute_image_embeddings(image_paths, batch_size=32)\n\n# L2 normalize\ndef l2norm(a):\n    if a.size == 0:\n        return a\n    norms = np.linalg.norm(a, axis=1, keepdims=True)\n    norms[norms==0] = 1.0\n    return a / norms\n\ntext_emb = l2norm(text_emb)\nimg_emb = l2norm(img_emb)\n\nprint(\"Shapes -> text:\", text_emb.shape, \"img:\", img_emb.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T04:07:08.756710Z","iopub.execute_input":"2025-10-18T04:07:08.757102Z","iopub.status.idle":"2025-10-18T04:07:11.699867Z","shell.execute_reply.started":"2025-10-18T04:07:08.757076Z","shell.execute_reply":"2025-10-18T04:07:11.698571Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (379 > 77). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Computing text embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in <cell line: 0>:47                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m44 \u001b[0mimage_paths = [ p.get(\u001b[33m\"\u001b[0m\u001b[33mlocal_image\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mfor\u001b[0m p \u001b[95min\u001b[0m products ]                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m45 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m46 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mComputing text embeddings...\u001b[0m\u001b[33m\"\u001b[0m)                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m47 text_emb = \u001b[1;4mcompute_text_embeddings(texts, batch_size=\u001b[0m\u001b[1;4;94m64\u001b[0m\u001b[1;4m)\u001b[0m                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m48 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mComputing image embeddings...\u001b[0m\u001b[33m\"\u001b[0m)                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m49 \u001b[0mimg_emb = compute_image_embeddings(image_paths, batch_size=\u001b[94m32\u001b[0m)                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m50 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in compute_text_embeddings:14                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(\u001b[94m0\u001b[0m, \u001b[96mlen\u001b[0m(texts), batch_size):                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatch = texts[i:i+batch_size]                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs = processor(text=batch, return_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m, padding=\u001b[94mTrue\u001b[0m).to(device)    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m14 \u001b[2m│   │   │   \u001b[0memb = \u001b[1;4mmodel.get_text_features(**inputs)\u001b[0m                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   │   │   \u001b[0mall_emb.append(emb.cpu().numpy())                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m all_emb:                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m np.vstack(all_emb)                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/\u001b[0m\u001b[1mmodeling_clip.py\u001b[0m:1004 in        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m get_text_features                                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1001 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1002 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1003 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1004 \u001b[2m│   │   \u001b[0mtext_outputs = \u001b[1;4;96mself\u001b[0m\u001b[1;4m.text_model(\u001b[0m                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1005 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4minput_ids=input_ids,\u001b[0m                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1006 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4mattention_mask=attention_mask,\u001b[0m                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1007 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4mposition_ids=position_ids,\u001b[0m                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1mmodule.py\u001b[0m:1739 in _wrapped_call_impl    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1736 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1737 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type: ignore[misc]\u001b[0m        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1738 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1739 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._call_impl(*args, **kwargs)\u001b[0m                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1740 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1741 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# torchrec tests the code consistency with the following code\u001b[0m                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1742 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# fmt: off\u001b[0m                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1mmodule.py\u001b[0m:1750 in _call_impl            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1747 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1748 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1749 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1750 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mforward_call(*args, **kwargs)\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1751 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1752 \u001b[0m\u001b[2m│   │   \u001b[0mresult = \u001b[94mNone\u001b[0m                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1753 \u001b[0m\u001b[2m│   │   \u001b[0mcalled_always_called_hooks = \u001b[96mset\u001b[0m()                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/\u001b[0m\u001b[1mmodeling_clip.py\u001b[0m:699 in forward \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 696 \u001b[0m\u001b[2m│   │   \u001b[0minput_shape = input_ids.size()                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 697 \u001b[0m\u001b[2m│   │   \u001b[0minput_ids = input_ids.view(-\u001b[94m1\u001b[0m, input_shape[-\u001b[94m1\u001b[0m])                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 698 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 699 \u001b[2m│   │   \u001b[0mhidden_states = \u001b[1;4;96mself\u001b[0m\u001b[1;4m.embeddings(input_ids=input_ids, position_ids=position_ids)\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 700 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 701 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# CLIP's text model uses causal mask, prepare it here.\u001b[0m                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 702 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/c\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1mmodule.py\u001b[0m:1739 in _wrapped_call_impl    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1736 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1737 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type: ignore[misc]\u001b[0m        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1738 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1739 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._call_impl(*args, **kwargs)\u001b[0m                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1740 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1741 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# torchrec tests the code consistency with the following code\u001b[0m                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1742 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# fmt: off\u001b[0m                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/\u001b[0m\u001b[1mmodule.py\u001b[0m:1750 in _call_impl            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1747 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1748 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1749 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1750 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mforward_call(*args, **kwargs)\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1751 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1752 \u001b[0m\u001b[2m│   │   \u001b[0mresult = \u001b[94mNone\u001b[0m                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1753 \u001b[0m\u001b[2m│   │   \u001b[0mcalled_always_called_hooks = \u001b[96mset\u001b[0m()                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/\u001b[0m\u001b[1mmodeling_clip.py\u001b[0m:222 in forward \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 219 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs_embeds = \u001b[96mself\u001b[0m.token_embedding(input_ids)                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 220 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 221 \u001b[0m\u001b[2m│   │   \u001b[0mposition_embeddings = \u001b[96mself\u001b[0m.position_embedding(position_ids)                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 222 \u001b[2m│   │   \u001b[0membeddings = \u001b[1;4minputs_embeds + position_embeddings\u001b[0m                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 223 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 224 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m embeddings                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 225 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mRuntimeError: \u001b[0mThe size of tensor a \u001b[1m(\u001b[0m\u001b[1;36m379\u001b[0m\u001b[1m)\u001b[0m must match the size of tensor b \u001b[1m(\u001b[0m\u001b[1;36m77\u001b[0m\u001b[1m)\u001b[0m at non-singleton dimension \u001b[1;36m1\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in &lt;cell line: 0&gt;:47                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44 </span>image_paths = [ p.get(<span style=\"color: #808000; text-decoration-color: #808000\">\"local_image\"</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> p <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> products ]                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Computing text embeddings...\"</span>)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>47 text_emb = <span style=\"font-weight: bold; text-decoration: underline\">compute_text_embeddings(texts, batch_size=</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">64</span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">48 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Computing image embeddings...\"</span>)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49 </span>img_emb = compute_image_embeddings(image_paths, batch_size=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">32</span>)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">50 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in compute_text_embeddings:14                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(texts), batch_size):                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>batch = texts[i:i+batch_size]                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>inputs = processor(text=batch, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>, padding=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>).to(device)    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>14 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>emb = <span style=\"font-weight: bold; text-decoration: underline\">model.get_text_features(**inputs)</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>all_emb.append(emb.cpu().numpy())                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> all_emb:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> np.vstack(all_emb)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/transformers/models/clip/</span><span style=\"font-weight: bold\">modeling_clip.py</span>:1004 in        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> get_text_features                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1001 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1002 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1003 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1004 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>text_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.text_model(</span>                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1005 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">input_ids=input_ids,</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1006 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">attention_mask=attention_mask,</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1007 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">position_ids=position_ids,</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/torch/nn/modules/</span><span style=\"font-weight: bold\">module.py</span>:1739 in _wrapped_call_impl    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1736 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1737 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[misc]</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1738 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1739 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">._call_impl(*args, **kwargs)</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1741 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># torchrec tests the code consistency with the following code</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1742 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># fmt: off</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/torch/nn/modules/</span><span style=\"font-weight: bold\">module.py</span>:1750 in _call_impl            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1747 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1748 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1749 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1750 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"font-weight: bold; text-decoration: underline\">forward_call(*args, **kwargs)</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1751 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1752 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1753 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>called_always_called_hooks = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">set</span>()                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/transformers/models/clip/</span><span style=\"font-weight: bold\">modeling_clip.py</span>:699 in forward <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 696 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>input_shape = input_ids.size()                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 697 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>input_ids = input_ids.view(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, input_shape[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>])                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 698 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 699 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.embeddings(input_ids=input_ids, position_ids=position_ids)</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 700 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 701 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># CLIP's text model uses causal mask, prepare it here.</span>                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 702 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/c</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/torch/nn/modules/</span><span style=\"font-weight: bold\">module.py</span>:1739 in _wrapped_call_impl    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1736 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1737 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[misc]</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1738 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1739 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">._call_impl(*args, **kwargs)</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1741 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># torchrec tests the code consistency with the following code</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1742 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># fmt: off</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/torch/nn/modules/</span><span style=\"font-weight: bold\">module.py</span>:1750 in _call_impl            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1747 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1748 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1749 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1750 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"font-weight: bold; text-decoration: underline\">forward_call(*args, **kwargs)</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1751 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1752 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1753 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>called_always_called_hooks = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">set</span>()                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/usr/local/lib/python3.11/dist-packages/transformers/models/clip/</span><span style=\"font-weight: bold\">modeling_clip.py</span>:222 in forward <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 219 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>inputs_embeds = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.token_embedding(input_ids)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 220 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 221 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>position_embeddings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.position_embedding(position_ids)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 222 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>embeddings = <span style=\"font-weight: bold; text-decoration: underline\">inputs_embeds + position_embeddings</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 223 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 224 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> embeddings                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 225 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>The size of tensor a <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">379</span><span style=\"font-weight: bold\">)</span> must match the size of tensor b <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span><span style=\"font-weight: bold\">)</span> at non-singleton dimension <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n</pre>\n"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# Combine vectors: weighted average (text-heavy by default)\nw_text = 0.6\nw_img = 0.4\n\ncombined = []\nids = []\nmetadata = {}\n\nfor i, p in enumerate(products):\n    te = text_emb[i] if text_emb.shape[0] > i else np.zeros((model.config.projection_dim,), dtype=np.float32)\n    ie = img_emb[i] if img_emb.shape[0] > i else np.zeros((model.config.projection_dim,), dtype=np.float32)\n    if p.get(\"local_image\") is None:\n        v = te\n    else:\n        v = (w_text * te + w_img * ie)\n    v = v / (np.linalg.norm(v) + 1e-12)\n    combined.append(v.astype(np.float32))\n    ids.append(str(p[\"id\"]))\n    # metadata should include all given columns (drop local path)\n    meta = p[\"meta\"].copy()\n    meta.pop(\"local_image\", None)\n    meta.pop(\"images_field\", None)\n    metadata[str(p[\"id\"])] = meta\n\nmatrix = np.vstack(combined)\nprint(\"Combined matrix shape:\", matrix.shape)\n\nOUT_DIR = WORK_DIR / \"index\"\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nindex_path = OUT_DIR / \"index.npz\"\nmeta_path = OUT_DIR / \"index.npz.meta.json\"\n\nnp.savez_compressed(str(index_path), ids=np.array(ids, dtype=object), matrix=matrix)\nwith open(meta_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(metadata, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved index:\", index_path)\nprint(\"Saved metadata:\", meta_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}